"use strict";(function(){const t={cache:!0};t.doc={id:"id",field:["title","content"],store:["title","href"]};const e=FlexSearch.create("balance",t);window.bookSearchIndex=e,e.add({id:0,href:"/bigdata-bootcamp/docs/environment/",title:"Environment",content:""}),e.add({id:1,href:"/bigdata-bootcamp/docs/sessions/",title:"Sessions",content:""}),e.add({id:2,href:"/bigdata-bootcamp/docs/environment/env-local-docker/",title:"Docker in Local OS",content:` For the purpose of the environment normalization, we provide a simple docker image for you, which contains most of the software required by this course. We also provide a few scripts to install some optional packages. The whole progress would seem as follow:
Make sure you have enough resources: It requires at least 8GB of Physical RAM; 16GB or larger would be better It requires at least 15GB of hard disk storage Install a docker environment in the local machine Start Docker Service, pull images, and create an instance Just rock it! Destroy the containers and images if they are no longer needed Since this docker image integrated many related services for the course, it requires at least 4GB RAM for this virtual machine. If you can not meet the minimum requirement, the system could randomly kill one or a few processes due to resource limitation, which causes a lot of strange errors which is even unable to reproduce.
DO NOT TRY TO DO THAT.
Instead, you can use cloud platforms such as Azure.
0. System requirements You should have enough system resources if you plan to start a container in your local OS.
You need to reserve at least 4 GB RAM for Docker and some extra memory for the host machine. However, you can still start all the Hadoop-related services except Zeppelin, even if you only reserve 4GB for the virtual machine.
1. Install Docker Docker is a software providing operating-system-level virtualization, also known as containers, promoted by the company Docker, Inc.. Docker uses the resource isolation features of the Linux kernel such as cgroups and kernel namespaces, and a union-capable file system such as OverlayFS and others to allow independent &ldquo;containers&rdquo; to run within a single Linux instance, avoiding the overhead of starting and maintaining virtual machines (VMs). (from Wikipedia) Basically, you can treat docker as a lightweight virtual machine with pretty high performance.
The installation instructions for different operating systems are provided in the links below. You can also check the official documentation here to get the latest news and detailed explanations.
Install Docker In Linux Install Docker In macOS Install Docker In Microsoft Windows Once the docker is installed, you can start your docker services and launch your docker container using the following commands:
docker # a tool to control docker docker-machine # a tool that lets you install Docker Engine on virtual hosts and manage the hosts in remote docker-compose # a tool for defining and running multi-container Docker applications If we use VirtualBox + Windows/macOS, the theory is pretty clear: we created a Linux instance in &ldquo;virtual remote&rdquo;, and control it using docker-machine. If we want to operate the &ldquo;remote docker service&rdquo;, we need to prepare a set of environment variables so that we do not have to use the eval $(docker-machine env default) command to access its location every time. We can list it using the command:
docker-machine env default Command docker-machine has been removed from the later versions of Docker Desktop. If you still want to use the command, you can follow the directions here and manually install the corresponding packages.
However, we will not rely on this command in our tutorial and its installation is not required.
If you are using docker-machine, you cannot reach the port from virtual machine using ip address 127.0.0.1 (localhost). As an alternative, you can extract the IP using this command:
$ printenv | grep &#34;DOCKER_HOST&#34; DOCKER_HOST=tcp://192.168.99.100:2376 And visit 192.168.99.100 instead of 127.0.0.1 to access the network stream from the virtual machine.
If these environments are not settled, docker will try to connect to the default Unix socket file. As a Docker.app user, the file is:
$ ls -alh /var/run/docker.sock lrwxr-xr-x 1 root daemon 55B Feb 10 19:09 /var/run/docker.sock -&gt; /Users/yu/Library/Containers/com.docker.docker/Data/s60 $ ls -al /Users/yu/Library/Containers/com.docker.docker/Data/s60 srwxr-xr-x 1 yu staff 0 Feb 10 19:09 /Users/yu/Library/Containers/com.docker.docker/Data/s60 As a Linux user, the location is slightly different:
$ ls -al /var/run/docker.sock srw-rw---- 1 root root 0 Feb 11 11:35 /var/run/docker.sock A Linux user must add a &ldquo;sudo&rdquo; before command docker since he has no access to docker.sock as an ordinary user. 2. Run Docker image 2.1. Start the container The command to start the container used in the following tutorials is:
docker run -it --privileged=true \\ --cap-add=SYS_ADMIN \\ -m 8192m -h bootcamp.local \\ --name bigbox -p 2222:22 -p 9530:9530 -p 8888:8888\\ -v /:/mnt/host \\ sunlab/bigbox:latest \\ /bin/bash It may take a while for the system to download and extract the container.
In general, the syntax of docker run is
docker run [options] image[:tag|@digest] [command] [args] Below explains the options used above:
-p &lt;host-port&gt;:&lt;vm-port&gt; This option is used to map the TCP port vm-port in the container to port host-port on the Docker host. Currently, the ports are reserved to:
8888 - Jupyter Notebook 9530 - Zeppelin Notebook Once you have started the Zeppelin service, this service will keep listening port 9530 in docker. You will be able to visit this service using http://127.0.0.1:9530 or http://DOCKER_HOST_IP:9530. The remote IP depends on the Docker Service you are running, which is described above.
If you are using Linux or Docker.app in macOS, you can visit &ldquo;localhost:9530&rdquo;, or other ports if you changed host-port If you are using VirtualBox + macOS or Windows, you should get the Docker&rsquo;s IP first -v, --volume=[host-src:]container-dest[:&lt;options&gt;] This option is used to mount a volume. Currently, we are using -v /:/mnt/host. In this case, we can visit the root of your file system for your host machine. If you are using macOS, /mnt/host/Users/&lt;yourname&gt;/ would be the $HOME of your MacBook. If you are using Windows, you can reach your C: disk from /mnt/host/c in docker.
Variable host-src accepts absolute path only.
-it -i : Keep STDIN open even if not attached -t : Allocate a pseudo-tty -h bootcamp.local Once you enter this docker environment, you can ping this docker environment itself as bootcamp.local. This variable is used in some configuration files for Hadoop ecosystems.
-m 8192m Memory limit (format: &lt;number&gt;[&lt;unit&gt;]). The number should be a positive integer. The unit can be one of b, k, m, or g.
This docker image requires at least 4G of RAM, while 8G is recommended when your physical machine has more than 8G of RAM.
The local machine is not the same as the remote server. If you are launching a remote server with 8G RAM, you can set this number as 7G.
Please refer to the official documentation for more information and the detailed explanations.
2.2. Start all necessary services In your terminal, you will generally meet two kinds of prompts.
[root@bootcamp /]# whoami # this prompt &#39;#&#39; indices you are root aka the administrator of this environment now root [yu@bootcamp /]$ whoami # this prompt &#39;$&#39; indices you are an ordinary user now yu You are in the sudo mode by default. We also assume that you are always in the sudo mode in the following discussions.
/scripts/start-services.sh This script helps you to start the necessary services for the Hadoop ecosystems.
You probably will encounter the &ldquo;Connection Refused&rdquo; exception if you forget to start these services. If you wish to host Zeppelin, you need to install it first by using the command:
/scripts/install-zeppelin.sh And start the service with the command:
/scripts/start-zeppelin.sh Then, Zeppelin will listen to the port 9530 Please refer to the Zeppelin tutorial for more information about the configuration and usage of Zeppelin Notebook.
If you wish to host Jupyter, you can start it by using the command:
/scripts/start-jupyter.sh Jupyter will listen to the port 8888
2.3. Stop all services You can stop the running services with:
/scripts/stop-services.sh 2.4. Detach or Exit To detach and suspend a Docker instance from the terminal, use the command:
ctrl + p, ctrl + q To exit,
exit 2.5. Re-attach If you detached an instance and want to re-attach it, you need to check the CONTAINER ID or NAMES of it first.
$ docker ps -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 011547e95ef5 sunlab/bigbox:latest &#34;/tini -- /bin/bash&#34; 6 hours ago Up 4 seconds 0.0.0.0:8888-&gt;8888/tcp, 0.0.0.0:9530-&gt;9530/tcp, 0.0.0.0:2222-&gt;22/tcp bigbox If the &ldquo;STATUS&rdquo; column is similar to &ldquo;Exited (0) 10 hours ago&rdquo;, you can restart the container:
$ docker start &lt;CONTAINER ID or NAMES&gt; And attach it with:
$ docker attach &lt;CONTAINER ID or NAMES&gt; Every time you restart your container, you need to re-start the services (section 2.2) before any HDFS related operations.
2.6. Destroy instance If you want to permanently remove the container:
$ docker rm &lt;CONTAINER ID or NAMES&gt; 2.7. Destroy images If you want to permanently remove any images, you need to list images first:
$ docker images REPOSITORY TAG IMAGE ID CREATED SIZE sunlab/bigbox latest bfd258e00de3 16 hours ago 2.65GB And remove them by REPOSITORY or IMAGE ID using the command:
$ docker rmi &lt;REPOSITORY or IMAGE ID&gt; 2.8. Update images $ docker pull sunlab/bigbox 2.9. Official documentations Please refer to this link for the introduction of images, containers, and storage drivers.
2.10. Optional: use docker-compose Docker Compose is a tool for defining and running multi-container Docker applications. A simple docker-compose.yml could simplify the parameters and make your life easier.
Please refer to this page for further instructions.
3. Configurations and logs 3.1. System Configurations $ cat /proc/meminfo | grep Mem ## Current Memory MemTotal: 8164680 kB ## Note: This value shoud no less than 4GB MemFree: 175524 kB MemAvailable: 5113340 kB $ cat /proc/cpuinfo | grep &#39;model name&#39; | head -1 ## CPU Brand model name	: Intel(R) Core(TM) i7-7920HQ CPU @ 3.10GHz $ cat /proc/cpuinfo | grep &#39;model name&#39; | wc -l ## CPU Count 4 $ df -h ## List Current Hard Disk Usage Filesystem Size Used Avail Use% Mounted on overlay 32G 4.6G 26G 16% / tmpfs 64M 0 64M 0% /dev ... $ ps -ef ## List Current Running Process UID PID PPID C STIME TTY TIME CMD root 1 0 0 01:38 pts/0 00:00:00 /tini -- /bin/bash root 7 1 0 01:38 pts/0 00:00:00 /bin/bash root 77 1 0 01:43 ? 00:00:00 /usr/sbin/sshd zookeep+ 136 1 0 01:43 ? 00:00:14 /usr/lib/jvm/java-openjdk/bin/java -Dzookeeper.log.dir=/var/log/zookeeper -Dzookeeper.root.logger=INFO,ROLLINGFILE -cp /usr/lib/zookeeper/bin/../build/classes:/ yarn 225 1 0 01:43 ? 00:00:13 /usr/lib/jvm/java/bin/java -Dproc_proxyserver -Xmx1000m -Dhadoop.log.dir=/var/log/hadoop-yarn -Dyarn.log.dir=/var/log/hadoop-yarn -Dhadoop.log.file=yarn-yarn-pr ... $ lsof -i:9530 ## Find the Process Listening to Some Specific Port COMMAND PID USER FD TYPE DEVICE SIZE/OFF NODE NAME java 3165 zeppelin 189u IPv4 229945 0t0 TCP *:9530 (LISTEN) 3.2. Logs hadoop-hdfs &ndash; /var/log/hadoop-hdfs/* hadoop-mapreduce &ndash; /var/log/hadoop-mapreduce/* hadoop-yarn &ndash; /var/log/hadoop-yarn/* hbase &ndash; /var/log/hbase/* hive &ndash; /var/log/hive/* spark &ndash; /var/log/spark/* zookeeper &ndash; /var/log/zookeeper/* zeppelin &ndash; /usr/local/zeppelin/logs/* `}),e.add({id:3,href:"/bigdata-bootcamp/docs/",title:"Docs",content:""}),e.add({id:4,href:"/bigdata-bootcamp/docs/environment/env-local-docker-linux/",title:"Install Docker in Linux",content:` Reference: [original website, official tutorial]
1. Install Docker on RHEL/CentOS/Fedora Get Docker CE for CentOS Get Docker CE for Fedora In brief, you can install Docker and start the service with the following commands:
sudo yum install docker-ce -y # install docker package sudo service docker start # start docker service chkconfig docker on # start-up automatically FAQ If your SELinux and BTRFS are on working, you may meet an error message as follow:
# systemctl status docker.service -l ... SELinux is not supported with the BTRFS graph driver! ... Modify /etc/sysconfig/docker as follow:
# Modify these options if you want to change the way the docker daemon runs #OPTIONS=&#39;--selinux-enabled&#39; OPTIONS=&#39;&#39; ... Restart your docker service
Storage Issue:
Error message found in /var/log/upstart/docker.log
[graphdriver] using prior storage driver \\&#34;btrfs\\&#34;... Just delete directory /var/lib/docker and restart the Docker service
2. Install Docker on Ubuntu/Debian Get Docker CE for Ubuntu Get Docker CE for Debian Generally, you can add the repository and execute
sudo apt-get install docker-ce Both Debian Series and RHEL Series can be controlled by
sudo service docker start # stop, restart, ... Once you started your service, you would find a socket file /var/run/docker.sock, and then you are able to execute your docker commands.
`}),e.add({id:5,href:"/bigdata-bootcamp/docs/environment/env-local-docker-macos/",title:"Install Docker in macOS",content:` References: [original website, official tutorial].
Currently, there are at least two approaches to running Docker services on macOS.
1. Option One: Docker.app We recommend this installation approach. You can visit the official website to download Docker Desktop and installation instructions. Select and download a proper version of docker image and drag it to your &ldquo;Applications&rdquo; folder to install Docker software. After installation, you can click the Docker icon in the toolbar and set the maximum memory to 4G-8G, as recommended.
Docker.app requires sudo access, and the container data are stored at $HOME/Library/Containers/com.docker.docker.
2. Option Two: Homebrew + VirtualBox + Docker However, here is an alternative solution.
First of all, you should make sure you have already installed HomeBrew. Secondly, you are supposed to make sure your brew is up-to-date.
brew update # update brew repository brew upgrade # update all packages for brew brew doctor # check your brew status Finally, you can install VirtualBox and Docker by using the following commands:
brew install Caskroom/cask/virtualbox brew install docker-machine brew install docker To keep the Docker service active, we can use brew&rsquo;s service manager
$ brew services start docker-machine==&gt; Successfully started \`docker-machine\` (label: homebrew.mxcl.docker-machine) Check the status:
$ brew services listName Status User Plistdocker-machine started name /Users/name/Library/LaunchAgents/homebrew.mxcl.docker-machine.plist Create a default instance using the following command:
docker-machine create --driver virtualbox --virtualbox-memory 8192 default Please refer to this link for detailed instructions.
Each time you create a new terminal window, you need to execute the following command before you use any docker commands docker *:
eval $(docker-machine env default) This command appends some environment variables to your current sessions.
FAQ Q: Can not connect to Docker
Error Message:
$ docker ps -aCannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running? Please make sure you have already started your session.
Q: Start docker-machine Failed, Can Not Get IP Address
The default manager conflicts with vpn AnyConnect. Please disconnect your AnyConnect VPN before starting the Docker server. Whether Docker is compatible with GlobalProtect VPN has not been tested yet.
Q: Invalid Active Developer Path
xcrun: error: invalid active developer path (/Library/Developer/CommandLineTools), missing xcrun at: /Library/Developer/CommandLineTools/usr/bin/xcrunError: Failure while executing: git config --local --replace-all homebrew.analyticsmessage true try xcode-select --install and then brew update, brew upgrade, and brew doctor again
Q: Where are the data for the images and hard disks?
They are in $HOME/.docker
`}),e.add({id:6,href:"/bigdata-bootcamp/docs/environment/env-local-docker-windows/",title:"Install Docker in Mircosoft Windows",content:` References: [original website, official tutorial].
1. Windows Docker Desktop (recommended) Before installation, please check this page for system requirements. If your system does not fulfill the prerequisite, you may see the image as follow. Please use Docker Toolbox on Windows instead.
Download the image from this link and follow the installer step by step.
Once you have successfully installed docker desktop, you may click the button &ldquo;Docker Desktop&rdquo;. It may take a few minutes to start the service. You may keep a watch on the whale on the right button.
Double click the whale button, and we can find a window to modify some of the properties.
Please go to the advanced tab and click the drivers you wish to share with the docker container. Note: if your homework is located on Disk-D, you may not select Disk-C, this could make your operation system safer.
Go to the advanced tab, and you can edit the maximum memory used by docker.
If you can execute the command docker ps -a and docker-compose without any error message returned, your configuration is successful.
2. Docker Toolbox on Windows You may install Docker Toolbox on Windows instead.
Going to the instruction page, click &ldquo;Get Docker Toolbox for Windows&rdquo;, you will download an installer. You are supposed to install Docker and VirtualBox during this wizard.
Click &ldquo;Docker Quickstart Terminal&rdquo;, you should be able to start a bash session. Close it, click virtual box. You may find there is one virtual machine is running. Close this machine, update the maximum base memory.
Click the &ldquo;Docker Quickstart Terminal&rdquo; and your docker is ready.
FAQ Q: VirtualBox will not boot a 64bits VM when Hyper-V is activated
You may meet message as follow:
Error with pre-create check: &ldquo;This computer is running Hyper-V. VirtualBox won&rsquo;t boot a 64bits VM when Hyper-V is activated. Either use Hyper-V as a driver, or disable the Hyper-V hypervisor. (To skip this check, use &ndash;virtualbox-no-vtx-check)
You can not run VirtualBox on a system with Hyper-V enabled. Hyper-V is a tier-1 hypervisor, which does not accept other hypervisors (from here)
It seems like Docker for Windows has already resolved this issue Try to disable Hyper-V. Caution: According to some reports, this operation may damage his/her network and had to reinstall all network adapters to get Internet back, or even getting a blue screen error. Try to use Hyper-V as your backend driver. https://docs.docker.com/machine/drivers/hyper-v/ `}),e.add({id:7,href:"/bigdata-bootcamp/docs/sessions/python-for-data-analysis/",title:"Python Tools for Data Analysis",content:`1. Python Installation: Anaconda If you don&rsquo;t feel like using the local environment, you can try Google Colab for a free online python environment. The examples are also available on Colab:
Ignore this session if you already have a python environment.
Anaconda is a complete, open source data science package with a community of over 6 million users. It is easy to download and install; and it supports Linux, macOS, and Windows (source).
In this tutorial, we&rsquo;ll use Miniconda for minimal installation. Please refer to this page for the difference between Anaconda and Miniconda and which one to choose.
1.1. Windows and macOS Download the latest Miniconda installer (with Python 3.9) from the official website. Install the package according to the instructions. Start to use conda environment with Anaconda Prompt or other shells if you enabled this feature during installation. Notice: To use conda command in other shells/prompts, you need to add the conda directory to your PATH environment variable.
Reference: [1, 2].
1.2. Linux with terminal Start the terminal. Switch to ~/Download/ with command cd ~/Download/. If the path does not exist, create one using mkdir ~/Download/. Download the latest Linux Miniconda distribution using wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh. Once download is complete, install Miniconda using bash Miniconda3-latest-Linux-x86_64.sh. Follow the prompts on the installer screens. If you are unsure about any setting, accept the defaults. You can change them later. To make the changes take effect, close and then re-open your terminal window or use the command source ~/.bashrc. If you are using zsh or other shells, make sure conda is initiated. To do this, switch back to bash and type the command conda init &lt;shell name&gt;. Reference: [1].
1.3. Verify your installation You can use the command conda list to check your conda installation. If the terminal returns a bunch of python packages, then your installation is successful.
Reference: [1].
1.4. Conda environment With conda, you can create, remove, and update environments, each with an independent version of Python interpreter and Python packages. Switching or moving between environments is called activating the environment.
This part is not necessary as you can directly base environment, which is the default conda environment. For those who want to know more, please refer to conda: managing environments for details and instructions.
2. Package Installation If you are using Anaconda or Miniconda, you can use the Anaconda package manager conda. You can also use other managers such as pip when the packages are not provided by any conda sources. However, in this tutorial, we&rsquo;ll only cover how to install packages with conda instructions.
To look for a specific package, you can visit this website and type the name of that package in the search box. For today&rsquo;s instruction, we need to install numpy, matplotlib, scikit-learn and pandas.
First, switch to your conda environment using conda activate &lt;env name&gt; (not necessary if you are using the default base environment), then install those packages by executing these instructions:
conda install -c conda-forge numpy conda install -c conda-forge matplotlib conda install -c conda-forge scikit-learn conda install -c conda-forge pandas The package manager will automatically install all dependencies. So if you choose to install scikit-learn first, then you don&rsquo;t have to install numpy manually as scikit-learn depends on numpy.
If you prefer a fancier and more powerful python shell, you can choose to install ipython and even jupyter notebook, which allows you to run your commands in your browser.
conda install -c conda-forge ipython conda install jupyter 3. Basic Python Concepts A more comprehensive tutorial can be found at the Stanford CS231n website. In this and the following sections, we&rsquo;ll introduce the basic concepts due to time limitations.
We use Python 3.9 in this tutorial.
Notice that previous Python interpreter versions may behave differently. Please refer to the official document for more details.
First, in your terminal, type python or ipython or jupyter notebook to start an interactive python shell. ipython or jupyter notebook is recommended.
3.1. Variable definition, input and output (print) There&rsquo;s no type constraint for a variable, i.e., a variable can be of any type.
a = 123 b = &#39;123&#39; c = &#34;1234&#34; print(a, b, c, type(a), type(b), type(c)) A variable can be overwritten by different types
a = 123.456 print(type(a)) a = &#39;123&#39; print(type(a)) Input some strings interactively:
x = input(&#39;Input something: &#39;) print(x, type(x)) Notice that this input method is rarely used in big data scenarios. A more practical input method is argparse.
3.2. List, tuple, set and dictionary List is a collection that is ordered and changeable. It allows duplicate members. Tuple is a collection which is ordered but not changeable. It also allows duplicate members. Set is a collection which is unordered and unindexed. It does not allow duplicate members. Elements in a set cannot be retrieved by index. Dictionary is a collection which is ordered, changeable and indexed. It does not allow duplicate members. Notice that Dictionary used to be unordered before Python 3.7.
_list = [1, 2, 1.2, &#39;1&#39;, &#39;2&#39;, 1] # this is a list _tuple = (1, 2, 1.2, &#39;1&#39;, &#39;2&#39;, 1) # this is a tuple _set = {1, 2, 1.2, &#39;1&#39;, &#39;2&#39;, 1} # this is a set _dict = { # this is a dict 1: &#39;111&#39;, 2: &#39;222&#39;, &#39;1&#39;: 567, 2.2: [&#39;J&#39;, &#39;Q&#39;, &#39;K&#39;] } print(_list, &#39;\\n&#39;, _tuple, &#39;\\n&#39;, _set, &#39;\\n&#39;, _dict) Access elements
print(_list[0], _list[-2], _list[1: 3]) print(_tuple[1], _tuple[-2]) print(_set[0], _set[-1]) print(_dict[1], _dict[&#39;1&#39;], _dict[2.2]) Shallow copy
a = _list a[0] = 888 print(a, &#39;\\n&#39;, _list) 3.3. If else if 888 not in _dict.keys(): _dict[888] = &#39;???&#39; elif 999 not in _dict.keys(): _dict[999] = &#39;!@#$%&#39; else: _dict[&#39;qwert&#39;] = &#39;poiuy&#39; 3.4. Loops for loop:
for x in _list: print(x) for i in range(len(_list)): print(_list[i]) while loop:
i = 0 while i != len(_list): print(_list[i]) i += 1 3.5 Function Define a function:
def my_func(x): x += 1 print(&#39;in function: &#39;, x) return x Call a function
t = 10 tt = my_func(t) print(f&#39;out of funciton, t: {t}, tt: {tt}&#39;) 4. Basic Numpy Usage 4.1. Array creation A numpy array is a grid of values, all of the same type, and is indexed by a tuple of nonnegative integers. The number of dimensions is the rank of the array; the shape of an array is a tuple of integers giving the size of the array along each dimension.
We can initialize numpy arrays from nested Python lists, and access elements using square brackets:
import numpy as np a = np.array([1, 2, 3]) # Create a rank 1 array print(type(a), a.dtype) print(a.shape) print(a[1]) b = np.array([[1,2,3],[4,5,6]]) # Create a rank 2 array print(b.shape) print(b[0, 0], b[0, 1], b[1, 0]) Change the type of an array:
print(a.dtype) a = a.astype(float) print(a.dtype) Other array creation methods:
a = np.zeros((2,2)) # Create an array of all zeros print(a) b = np.ones((1,2)) # Create an array of all ones print(b) c = np.full((2,2), 7, dtype=np.float32) # Create a constant array print(c) d = np.eye(3) # Create a 3x3 identity matrix print(d) e = np.random.random((3,3)) # Create an array filled with random values print(e) 4.2. Array indexing Similar to Python lists, numpy arrays can be sliced. Since arrays may be multidimensional, you must specify a slice for each dimension of the array:
# Create a rank 1 array and reshape it to a 3x4 matrix a = np.arange(12).reshape(3, 4) b = a[:2, 1:3] print(a) print(b) # Shallow copy b[0, 0] = 888 print(a) You can mix integer indexing with slice indexing. However, integer indexing will yield an array of lower rank than the original array:
row_r1 = a[1, :] # Rank 1 view of the second row of a row_r2 = a[1:2, :] # Rank 2 view of the second row of a print(row_r1, row_r1.shape) print(row_r2, row_r2.shape) You can also access element in the array through lists:
x = [0, 1, 2] y = [3, 1, 0] print(a[x, y]) Or through a boolean array:
b = a &gt; 4 print(b) print(a[b]) 4.3. Array math Basic mathematical functions operate element-wise on arrays, and are available both as operator overloads and as functions in the numpy module:
x = np.arange(1, 5, dtype=np.float).reshape(2, 2) y = np.arange(5, 9, dtype=np.float).reshape(2, 2) print(x) print(y) # Elementwise sum print(x + y) print(np.add(x, y)) # Elementwise difference print(x - y) print(np.subtract(x, y)) # Elementwise product print(x * y) print(np.multiply(x, y)) # Elementwise division print(x / y) print(np.divide(x, y)) # Elementwise square print(x ** 2) print(np.power(x, 2)) # Elementwise square root print(x ** 0.5) print(np.sqrt(x)) Matrix multiplication
x = np.arange(1, 5, dtype=np.float).reshape(2, 2) y = np.arange(5, 9, dtype=np.float).reshape(2, 2) print(x) print(y) v = np.array([9, 10], dtype=np.float) w = np.array([11, 12], dtype=np.float) # Inner product print(v.dot(w)) print(np.dot(v, w)) print(v @ w) # Matrix / vector product print(x.dot(v)) print(np.dot(x, v)) print(x @ v) # Matrix / matrix product print(x.dot(y)) print(np.dot(x, y)) print(x @ y) Attention: np.dot() and @ behaves differently when the matrix rank is greater than 2.
Numpy provides many useful functions for performing computations on arrays such as sum:
print(np.sum(x)) # Compute sum of all elements; prints &#34;10&#34; print(x.sum()) # same as above print(np.sum(x, axis=0)) # Compute sum of each column; prints &#34;[4 6]&#34; print(np.sum(x, axis=1)) # Compute sum of each row; prints &#34;[3 7]&#34; To transpose a matrix, use the T attribute of an array object:
print(x.T) # Note that taking the transpose of a rank one array does nothing: print(v) print(v.T) 5. Using Matplotlib for visualization import numpy as np import matplotlib.pyplot as plt # %matplotlib qt # Compute the x and y coordinates for points on a sine curve x = np.arange(0, 3 * np.pi, 0.1) y = np.sin(x) # Plot the points using matplotlib plt.plot(x, y) plt.show() # You must call plt.show() to make graphics appear. Note: for jupyter notebook, you can use the command %matplotlib inline to make the graphics embedded in the editor or %matplotlib qt to make them pop out.
To plot multiple lines at once, and add a title, legend, and axis labels:
x = np.arange(0, 3 * np.pi, 0.1) y_sin = np.sin(x) y_cos = np.cos(x) # Plot the points using matplotlib plt.plot(x, y_sin) plt.plot(x, y_cos) plt.xlabel(&#39;x axis label&#39;) plt.ylabel(&#39;y axis label&#39;) plt.title(&#39;Sine and Cosine&#39;) plt.legend([&#39;Sine&#39;, &#39;Cosine&#39;]) plt.show() You can plot different things in the same figure using the subplot function. Here is an example:
# Set up a subplot grid that has height 2 and width 1, # and set the first such subplot as active. plt.subplot(2, 1, 1) # Make the first plot plt.plot(x, y_sin) plt.title(&#39;Sine&#39;) # Set the second subplot as active, and make the second plot. plt.subplot(2, 1, 2) plt.plot(x, y_cos) plt.title(&#39;Cosine&#39;) # Show the figure. plt.show() 6. Pandas and Scikit-Learn for Data Science In this section, we will look at a data science example using pandas as data management tool and scikit-learn (sklearn) as algorithm implementation. This section is modified from this tutorial.
6.1. Import packages import numpy as np import pandas as pd # automatically split the data into training and test set from sklearn.model_selection import train_test_split from sklearn import preprocessing # classifiers and regressors from sklearn.ensemble import RandomForestRegressor # Construct a Pipeline from the given estimators from sklearn.pipeline import make_pipeline # Exhaustive search over specified parameter values for an estimator. from sklearn.model_selection import GridSearchCV # Training objective and evaluation metrics from sklearn.metrics import mean_squared_error, r2_score # For model persistence # you can use \`from sklearn.externals import joblib\` if your sklearn version is earlier than 0.23 import joblib 6.2. Load data You can download the data by clicking the link or using wget: wget https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv and move the file to your current folder. Then, load the csv data into memory through pandas:
data = pd.read_csv(&#39;winequality-red.csv&#39;, sep=&#39;;&#39;) Or, you can directly load the data through URL.
dataset_url = &#39;https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv&#39; data = pd.read_csv(dataset_url, sep=&#39;;&#39;) You can also load datasets stored in other formats with pandas. A detailed document is at pandas: io.
6.3. Take a look of the loaded data The data loaded is stored in the type of pandas.core.frame.DataFrame
To give a peak of the data, we can use
print(data) This will return a nice-looking preview of the elements in the DataFrame.
To view the name of the features of a DataFrame, one can use
print(data.keys()) To access one column, i.e., all instances of a feature, e.g., pH, one can use
# These will return the same result print(data[&#39;pH&#39;]) print(data.pH) To access a row, you need the DataFrame.iloc attribute:
print(data.iloc[10]) We can also easily print some summary statistics:
print(data.describe()) 6.4. Split data First, let&rsquo;s separate our target (y) feature from our input (X) features and divide the dataset into training and test sets using the train_test_split function:
y = data.quality X = data.drop(&#39;quality&#39;, axis=1) X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.2, random_state=0, stratify=y ) Stratify your sample by the target variable will ensure your training set looks similar to your test set, making your evaluation metrics more reliable.
6.5. Pre-processing Standardization is the process of subtracting the means from each feature and then dividing by the feature standard deviations. It is a common requirement for machine learning tasks. Many algorithms assume that all features are centered around zero and have approximately the same variance.
scaler = preprocessing.StandardScaler().fit(X_train) X_train_scaled = scaler.transform(X_train) X_test_scaled = scaler.transform(X_test) # To prove the trainig and testing sets have (nearly) zero mean and one deviation print(X_train_scaled.mean(axis=0)) print(X_train_scaled.std(axis=0)) print(X_test_scaled.mean(axis=0)) print(X_test_scaled.std(axis=0)) 6.6. Fit the model If we do not need to fine-tune the hyperparameters, we can define a random forest regression model with the default hyperparameters and fit the model using
regr = RandomForestRegressor() regr.fit(X_train_scaled, y_train) To examine the performance, we use the test set to calculate the scores
pred = regr.predict(X_test_scaled) print(r2_score(y_test, pred)) print(mean_squared_error(y_test, pred)) 6.7. Define the cross-validation pipeline Fine-tuning hyperparameters is an important job in Machine Learning since a set of carefully chosen hyperparameters may greatly improve the performance of the model.
In practice, when we set up the cross-validation pipeline, we won&rsquo;t even need to manually fit the data. Instead, we&rsquo;ll simply declare the class object, like so:
pipeline = make_pipeline( preprocessing.StandardScaler(), RandomForestRegressor(n_estimators=100) ) To check the hyperparameters, we may use
print pipeline.get_params() or refer to the official document.
Now, let&rsquo;s declare the hyperparameters we want to tune through cross-validation.
hyperparameters = { &#39;randomforestregressor__max_features&#39; : [&#39;auto&#39;, &#39;sqrt&#39;, &#39;log2&#39;], &#39;randomforestregressor__max_depth&#39;: [None, 5, 3, 1] } Then, we can set a 10-fold cross validation as simple as
clf = GridSearchCV(pipeline, hyperparameters, cv=10) Finally, we can automatically fine-tune the model using
clf.fit(X_train, y_train) After the model fitting, if we want to check the best hyperparameters, we can use
print(clf.best_params_) Same as before, we evaluate the fitted model on test set
pred = clf.predict(X_test) print(r2_score(y_test, pred)) print(mean_squared_error(y_test, pred)) 6.8. Save and load models After training, we may want to save the trained model for future use. For this purpose, we can use
joblib.dump(clf, &#39;rf_regressor.pkl&#39;) When you want to load the model again, simply use this function:
clf2 = joblib.load(&#39;rf_regressor.pkl&#39;) # Predict data set using loaded model clf2.predict(X_test) A more comprehensive example of scikit-learn can be found here.
`}),e.add({id:8,href:"/bigdata-bootcamp/docs/environment/env-azure-docker/",title:"Env Azure Docker",content:`Docker in Azure We could use Azure as a virtual machine provider. If you have no enough resource to host our envornment in local, you can also launch an Azure instance, start a docker service inside, and host our docker image.
We can create a Docker on Ubuntu Server in Azure, and then pull image from hub.docker.com.
Launch an Azure instance Option 1: Launch a Pre-installed Docker Host &ldquo;Docker on Ubuntu Server&rdquo; is a container based on Ubuntu Server 16.04 LTS published by Canonical. You can launch a new ”Docker on Ubuntu Server” instance, and you will able to start your docker directly.
Open the Portal in Azure at https://portal.azure.com Click Virtual Machines on the left sidebar Click “+ ADD” to create a new instance Type “docker” in search box, and select &ldquo;Docker on Ubuntu Server&rdquo; Click “Create” on the introduction page Fill your host name, user name, authentication Click Pricing Tier, and choose D2S_V3 Click “create” to create the instance The D2S_V3 is just an example. You may choose anyone fulfill the requirement ( RAM may greater than 8G, CPU should more than 1). Option 2: Launch a clear linux some detail information of docker. &ldquo;Docker on Ubuntu Server&rdquo; is using &ldquo;classic deployment&rdquo;. It seems like there are too few choise in &ldquo;classic deployment&rdquo; now. If you wish to have different option. Or, if you are more familiar with CentOS, SUSE or some other distributions, you can simply choose them.
In this case, you could unlocked more options in Azure Instance Type. Whatever your choise is, it is just a docker container, which does not matter the detail in your environment.
Open the Portal in Azure at https://portal.azure.com Click Virtual Machines on the left sidebar Click “+ ADD” to create a new instance Type “CentOS”, or &ldquo;Ubuntu&rdquo; in search box, and select any image you like. For example, I would like to choose &ldquo;Ubuntu Server 16.04 LTS&rdquo; here You should able to find a drop down box between &ldquo;Select a deployment mode&rdquo; and &ldquo;Create&rdquo; which should has 2 options &ldquo;Resource Manager&rdquo; and &ldquo;Classic&rdquo;. Please make sure it is on &ldquo;Resource Manager&rdquo; Click “Create” on the introduction page Fill your host name, user name, authentication Click &ldquo;Ok&rdquo;, and you should see a few options for your virtual machine. For this course, I would suggest you to choose a instance has 8GB or 16GB RAM and 2-4 vCPUs Fill in the rest of the information by yourself, and finally click “create” to create the instance Connect to the instance Open the Portal in Azure at https://portal.azure.com Click All resources on left sidebar. Select your instances from “Virtual machines (classic)”, “Cloud service” or &ldquo;Virtual machines&rdquo; depends on your choise find Public IP addresses in “Overview” Login via command “ssh your-username@public-ip” in *nix or using putty for windows (If you are using option 2), Install Docker in Linux Start a docker container Most of the related application are already installed, you can also install other apps with command “apt-get”.
For example:
sudo apt-get install git tmux And then, start a new docker instance
sudo docker run -it --privileged=true \\ --cap-add=SYS_ADMIN \\ -m 6144m -h bootcamp.local \\ --name bigbox -p 2222:22 -p 9530:9530 -p 8888:8888\\ -v /:/mnt/host \\ sunlab/bigbox:latest \\ /bin/bash Login remotely and the corresponding security issue To access the service we launched in Azure, we are required to get the corresponding port opened. In default, the ports are closed. A good way to open it is change the rule of filewall. Please refer to this official document to open the port.
However, since this image was designed to start a easy learning environment, if you keep the ports open to the public, your environment could be hacked easily. You can also making full use of the SSH port forwarding.
For example, you can create a new terminal, and type a command as follow:
ssh -L 2222:localhost:2222 \\-L 8888:localhost:8888 \\your-azure-user@your-azure-host This command will connect to your azure VM. In the meantime, It will also forward the network steam from azure:{2222, 8888}. And then, you can visit localhost:8888 to visit jupiter if you have your jupiter in azure started.
The spending in Azure is calculated by time usage. Launch a better instance and fully destroy it instance once you finish your job will reduce your time in any dimension You may use tmux to make your life better `}),e.add({id:9,href:"/bigdata-bootcamp/docs/sessions/scala-basic/",title:"Scala Basic",content:`Scala Basics Learning Objectives
Learn how to work with Scala interactive shell. Understand var and val. Define variables, functions and classes, and make function calls Understand Simple Build Tool (SBT). In this section we will briefly go through the essential knowledge about Scala. You will first learn how to work with Scala shell, then learn how to use variables, functions with examples. Finally, we give instructions about how to compile and run a standalone program using sbt.
Scala Shell You can open a Scala shell by typing scala. Or, you can use sbt by typing sbt console. The second approach will help you add your project source code and dependencies into class path, so that your functions or library functions will be available for you to try to in the interactive shell. But in this training, we will stick to Scala shell for simplicity.
Once starting the Scala shell you will see
$ scala Welcome to Scala version 2.10.5 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0). Type in expressions to have them evaluated. Type :help for more information. scala&gt; You can type :quit to stop and quit the shell, but don&rsquo;t do that now :-) Next you will learn some Scala operations in the shell with the following materials.
Variables Declare val and var In Scala, there are two types of variable, immutable val and mutable var. Unlike some functional programming language that requires immutable variables, Scala allows existence of mutable variables but immutable is recommended as it is easier to verify the correctness of your program. Suppose you are still in the Scala interactive shell. Define an immutable variable as
scala&gt; val myInt = 1 + 1 myInt: Int = 2 scala&gt; myInt = 3 where val is a keyword in scala that makes the variables immutable. If you reassign a value to myInt, error will be reported.
scala&gt; myInt = 3 &lt;console&gt;:8: error: reassignment to val myInt = 3 ^ scala&gt; In interactive shell, it&rsquo;s possible to redefine variable with same name. In Scala source code file, it&rsquo;s not allowed.
scala&gt; val a = 1 a: Int = 1 scala&gt; val a = 2 a: Int = 2 Instead, variables declared with var are mutable. Ideally, we try to use val instead of var if possible as a good practice of functional programming.
You may have concern that maybe too many immutable variables will be declared. Actually, with chained function calls, that situation is not the case for well organized code. An example of mutable variable is
scala&gt; var myString = &#34;Hello Big Data&#34; myString: String = Hello Big Data scala&gt; myString = &#34;Hello Healthcare&#34; myString: String = Hello Healthcare Type Scala may seem like a script language like JavaScript or Python, as variable type is not specified explicitly. In fact, Scala is a static type language and the compiler can implicitly infer the type in most cases. However, you can always specify a type as
scala&gt; val myDouble: Double = 3 myDouble: Double = 3.0 It is always encouraged to specify the type so unless the type is too obvious.
Besides simple built-in variable types like Int, Double and String, you will also learn about List and Tuple in the training:
scala&gt; val myList: List[String] = List(&#34;this&#34;, &#34;is&#34;, &#34;a&#34;, &#34;list&#34;, &#34;of&#34;, &#34;string&#34;) myList: List[String] = List(this, is, a, list, of, string) scala&gt; val myTuple: (Double, Double) = (1.0, 2.0) myTuple: (Double, Double) = (1.0,2.0) Here the List[String] is syntax of generics in Scala, which is same as C#. In the above example, List[String] means a List of String. Similarly, (Double, Double) means a two-field tuple type and both the 1st element and the 2nd element should be of type Double.
Functions You can define a function and invoke the function like
scala&gt; def triple(x: Int): Int = { x*3 } triple: (x: Int)Int scala&gt; triple(2) res0: Int = 6 Where x: Int is parameter and its type, and the second Int is function return type. There&rsquo;s not explicit return statement, but the result of last expresssion x*3 will be returned (similar to some other programming languages like Ruby). In this example, as there is only one expression and return type can be infered by the compiler, you may define the function as
def triple(x: Int) = x*3 Scala is object-oriented (OO), function calls on a class method are straightforward like most OO languages (e.g. Java, C#)
scala&gt; myString = &#34;Hello Healthcare&#34; myString: String = Hello Healthcare scala&gt; myString.lastIndexOf(&#34;Healthcare&#34;) res1: Int = 6 If the function does not have parameters, you can even call it without parenthesis
scala&gt; val myInt = 2 scala&gt; myInt.toString res2: String = 2 You can also define an anonymous function and pass it to variable like a lambda expression in some other languages such as Python:
scala&gt; val increaseOne = (x: Int) =&gt; x + 1 increaseOne: Int =&gt; Int = &lt;function1&gt; scala&gt; increaseOne(3) res3: Int = 4 Anonymous function is very useful as it can be passed as a parameter to a function call
scala&gt; myList.foreach{item: String =&gt; println(item)} this is a list of string where item: String =&gt; println(item) is an anonymous function. This function call can be further simplified to
scala&gt; myList.foreach(println(_)) scala&gt; myList.foreach(println) where _ represents first parameter of the anonymous function with body println(_). Additional _ can be specified to represent more than one parameter. For example, we can calculate the total payment that a patient made by
scala&gt; val payments = List(1, 2, 3, 4, 5, 6) payments: List[Int] = List(1, 2, 3, 4, 5, 6) scala&gt; payments.reduce(_ + _) res0: Int = 21 In above example, reduce will aggregate List[V] into V and we defined the aggregator as _ + _ to sum them up. Of course, you can write that more explicitly like
scala&gt; payments.reduce((a, b) =&gt; a+b) res1: Int = 21 Here reduce is a construct from functional programming. It can be illustrated with the figure below where a function f is applied to one element at a time and the result together with next element will be parameters of the next function call until the end of the list.
It&rsquo;s important to remember that for reduce operation, the input is List[V] and the output is V. Interested reader can learn more from wiki. In contrast to reduce, you can of course write code using for loop, which is verbose and very rare in Scala,
scala&gt; var totalPayment = 0 totalPayment: Int = 0 scala&gt; for (payment &lt;- payments) { totalPayment += payment } scala&gt; totalPayment res2: Int = 21 scala&gt; Class Declaration of a class in Scala is as simple as
scala&gt; class Patient(val name: String, val id: Int) defined class Patient scala&gt; val patient = new Patient(&#34;Bob&#34;, 1) patient: Patient = Patient@755f5e80 scala&gt; patient.name res13: String = Bob Here we see the succinct syntax of Scala again. class Patient(val name: String, val id: Int) not only defines constructor of Patient but also defines two member variables (name and id).
A special kind of class that we will use a lot is Case Class. Case Class can be declared as
scala&gt; case class Patient(val name: String, val id: Int) and see below Pattern Matching for use case.
Pattern Matching You may know the switch..case in other languages. Scala provides a more flexible and powerful technique, Pattern Matching. Below example shows one can match by-value and by-type in one match.
val payment:Any = 21 payment match { case p: String =&gt; println(&#34;payment is a String&#34;) case p: Int if p &gt; 30 =&gt; println(&#34;payment &gt; 30&#34;) case p: Int if p == 0 =&gt; println(&#34;zero payment&#34;) case _ =&gt; println(&#34;otherwise&#34;) } It&rsquo;s very convenient to use case class in pattern matching
scala&gt; val p = new Patient(&#34;Abc&#34;, 1) p: Patient = Patient(Abc,1) scala&gt; p match {case Patient(&#34;Abc&#34;, id) =&gt; println(s&#34;matching id is $id&#34;)} matching id is 1 Here we not only matched p as Patient type, but also matched patient name and extracted one member field from the Patient class instance.
p match { case Patient(&#34;Abc&#34;, id) =&gt; println(s&#34;matching id is $id&#34;) case _ =&gt; println(&#34;not matched&#34;) } Standalone Program Working with large real-world applications, you usually need to compile and package your source code with some tools. Here we show how to compile and run a simple program with sbt. Run the sample code in &lsquo;hello-bigdata&rsquo; folder
% cd ~/bigdata-bootcamp/sample/hello-bigdata % sbt run Attempting to fetch sbt ######################################################################### 100.0% Launching sbt from sbt-launch-0.13.8.jar [info] ..... [info] Done updating. [info] Compiling 1 Scala source to ./hello-bigdata/target/scala-2.10/classes... [info] Running Hello Hello bigdata [success] Total time: 2 s, completed May 3, 2015 8:42:48 PM The source code file hello.scala is compiled and invoked.
Further Reading This is a very brief overview of important features from the Scala language. We highly recommend readers to checkout references below to get a better and more complete understanding of Scala programming language.
Twitter Scala School Official Scala Tutorial SBT tutorial `}),e.add({id:10,href:"/bigdata-bootcamp/docs/sessions/scala-intro/",title:"Scala Intro",content:`Scala Introduction Learning Objectives
Provide more details of scala language Basic Gramma Start using Scala After installed scala, you can type scala in command line and get result as follow:
$ scala Welcome to Scala version 2.11.7 Type in expressions to have them evaluated. Type :help for more information. scala&gt; println(&#34;Hello, World!&#34;) Hello, World! The synopsis of a varialbe is:
scala&gt; val i:String = &#34;abc&#34; i: String = abc val means it it is immutable variable, you can use &ldquo;var&rdquo; to define a mutable variable i is the name of this variable String is the type of this string, it can be omitted here &ldquo;abc&rdquo; is the value of this variable Define a function:
scala&gt; def foo(v0:Int, v1:Int):Int = { | println(v0 max v1) | v0 + v1 | } foo: (v0: Int, v1: Int)Int scala&gt; foo(1, 2) 2 res0: Int = 3 Define a class:
scala&gt; class Foo(a:String, b:Int) { | def length = a.length | } defined class Foo scala&gt; val foo:Foo = new Foo(&#34;Hello, World!&#34;, 3) foo: Foo = Foo@6438a396 scala&gt; println(foo.length) 13 Define a case class:
scala&gt; case class Foo(a:String, b:Int) defined class Foo scala&gt; val foo:Foo = Foo(a = &#34;Hello, World!&#34;, b = 3) foo: Foo = Foo(Hello, World!,3) scala&gt; println(foo.a) Hello, World! Differences between case class and class
Define an Object
scala&gt; object Foo { | def greeting() { | println(&#34;Greeting from Foo&#34;) | } | } defined object Foo scala&gt; Foo.greeting() Greeting from Foo Functions/variables in Object is similar to the static function and variable in Java.
What is ought to be highligted is the use of &ldquo;apply&rdquo;. SomeObject.apply(v:Int) equals SomeObject(v:Int)
scala&gt; :paste // Entering paste mode (ctrl-D to finish) case class Foo(a:String, b:Int) object Foo { def apply(a:String): Foo = Foo(a, a.length) } // Exiting paste mode, now interpreting. defined class Foo defined object Foo scala&gt; val foo = Foo(&#34;Hello, World!&#34;) foo: Foo = Foo(Hello, World!,13) Finally, we should know the usage of code block.
We can create a code block anywhere, and the last line is the result of this block.
For example,
def foo(i:Int) = { println(s&#34;value: $i&#34;) i * 2 } val newList = List[Int](1, 2, 3).map(i =&gt; foo(i)) We can use the follow lines instead:
val newList = List[Int](1, 2, 3).map(i =&gt; { println(s&#34;value: $i&#34;) i * 2 }) A better practice here is:
val newList = List[Int](1, 2, 3).map{i =&gt; println(s&#34;value: $i&#34;) i * 2 } Case Study of some Common Types Option, Some, None We can use null in Scala as null pointer, but it is not recommended. We are supposed to use Option[SomeType] to indicate this variable is optional.
We can assueme every variable without Option are not null pointer if we are not calling java code. This help us reduce a lot of code.
If we wanna to get a variable with Option, here are two method
val oi = Option(1) val i = oi match { case Some(ri) =&gt; ri case None =&gt; -1 } println(i) Besides, we can also use method &ldquo;isDefined/isEmpty&rdquo;.
val oi = Option(1) if(oi.isDefined) { println(s&#34;oi: \${oi.get}&#34;) } else { println(&#34;oi is empty&#34;) } What should be highlighted is Option(null) returns None, but Some(null) is Some(null) which is not equals None.
match is a useful reserved words, we can use it in various of situations
Firstly, we can use it as &ldquo;switch&rdquo; &amp; &ldquo;case&rdquo; in some other programming languages.
true match { case true =&gt; println(&#34;true&#34;) case false =&gt; println(&#34;false&#34;) } Secondly, we can find it by the type of the data,
case class A(i:Int,j:Double) case class B(a:A, k:Double) val data = B(A(1,2.0),3.14) data match { case B(A(_,v),_) =&gt; println(s&#34;required value: $v&#34;) case _ =&gt; println(&#34;match failed&#34;) } Given a case class B, but we only wish to retrievee the value B.a.j, we can use &ldquo;_&rdquo; as placeholder.
Common methods in List, Array, Set, and so on In scala, we always transfer the List( Array, Set, Map etc.) from one status to another status. the methods of
toList, toArray, toSet &ndash; convert each other
par &ndash; Parallelize List, Array and Map, the result of Seq[Int]().par is ParSeq[Int], you will able to process each element in parallel when you are using foreach, map etc., and unable to call &ldquo;sort&rdquo; before you are using &ldquo;toList&rdquo;.
distinct &ndash; Removes duplicate elements
foreach &ndash; Process each element and return nothing
List[Int](1,2,3).foreach{ i =&gt; println(i) } It wil print 1, 2, 3 in order
List[Int](1,2,3).par.foreach{ i =&gt; println(i) } It will print 1, 2, 3, but the order is not guaranteed.
map &ndash; Process each element and construct a List using return value List[Int](1,2,3).map{ i =&gt; i + 1} It will return List[Int](2,3,4)
The result of List[A]().map(some-oper-return-type-B) is List[B], while the result of Array[A]().map map is Array[B].
flatten &ndash; The flatten method takes a list of lists and flattens it out to a single list: scala&gt; List[List[Int]](List(1,2),List(3,4)).flatten res1: List[Int] = List(1, 2, 3, 4) scala&gt; List[Option[Integer]](Some(1),Some[Integer](null),Some(2),None,Some(3)).flatten res2: List[Integer] = List(1, null, 2, 3) flatMap &ndash; The flatMap is similar to map, but it takes a function returning a list of elements as its right operand. It applies the function to each list element and returns the concatenation of all function results. The result equals to map + flatten
collect &ndash; The iterator obtained from applying the partial function to every element in it for which it is defined and collecting the results.
scala&gt; List(1,2,3.4,&#34;str&#34;) collect { | case i:Int =&gt; (i * 2).toString | case f:Double =&gt; f.toString | } res0: List[String] = List(2, 4, 3.4) The function match elements in Int and Double, process them and return the value, but ignore string elements.
filter &ndash; Filter this list scala&gt; List(1,2,3).filter(_ % 2 == 0) res1: List[Int] = List(2) filterNot &ndash; Similar to filter scala&gt; List(1,2,3).filterNot(_ % 2 == 0) res2: List[Int] = List(1, 3) forall &ndash; Return true if All elements are return true by the partial function. It will immediately return once one element returns false, and ignore the rest elements. scala&gt; List(2,1,0,-1).forall{ i =&gt; | val res = i &gt; 0 | println(s&#34;$i &gt; 0? $res&#34;) | res | } 2 &gt; 0? true 1 &gt; 0? true 0 &gt; 0? false res0: Boolean = false exists &ndash; Return true if there are at least One element returns true. scala&gt; List(2,1,0,-1).exists{ i =&gt; | val res = i &lt;= 0 | println(s&#34;$i &lt;= 0? $res&#34;) | res | } 2 &lt;= 0? false 1 &lt;= 0? false 0 &lt;= 0? true res2: Boolean = true find &ndash; Return the first element returns true by the partial function. Return None if no elemet found. scala&gt; List(2,1,0,-1).find{ i =&gt; | val res = i &lt;= 0 | println(s&#34;$i &lt;= 0? $res&#34;) | res | } 2 &lt;= 0? false 1 &lt;= 0? false 0 &lt;= 0? true res3: Option[Int] = Some(0) sortWith &ndash; sort the elements scala&gt; List(1,3,2).sortWith((leftOne,rightOne) =&gt; leftOne &gt; rightOne) res5: List[Int] = List(3, 2, 1) zipWithIndex &ndash; List(&#34;a&#34;,&#34;b&#34;).zipWithIndex.foreach{ kv:(String,Int) =&gt; println(s&#34;k:\${kv._1}, v:\${kv._2}&#34;)} It will rebuild a List with index
k:a, v:0k:b, v:1 for - Scala&rsquo;s keyword for can be used in various of situations. Basically,
for{ i &lt;- List(1,2,3) } yield (i,i+1) It equals:
List(1,2,3).map(i =&gt; (i, i+1)) Besides,
for{ i &lt;- List(1,2,3) j &lt;- List(4,5,6) } yield (i,j) We will get the cartesian product of List(1,2,3) and List(4,5,6): List((1,4), (1,5), (1,6), (2,4), (2,5), (2,6), (3,4), (3,5), (3,6))
We can add a filter in condition:
for{ i &lt;- List(1,2,3) if i != 1 j &lt;- List(4,5,6) if i * j % 2 == 1 } yield (i,j) the result is : List((3,5))
Another usage of for is as follow:
Let&rsquo;s define variables as follow:
val a = Some(1)val b = Some(2)val c = Some(3) We can execute like this:
for { i &lt;- a j &lt;- b k &lt;- c r &lt;- { println(s&#34;i: $i, j:$j, k:$k&#34;) Some(i * j * k) } } yield r The response is:
i: 1, j:1, k:3 res9: Option[Int] = Some(6) Let&rsquo;s define b as None:
scala&gt; val b:Option[Int] = None b: Option[Int] = None scala&gt; for { | i &lt;- a | j &lt;- b | k &lt;- c | r &lt;- { | println(s&#34;i: $i, j:$j, k:$k&#34;) | Some(i * j * k) | } | } yield r res14: Option[Int] = None while - Similar to while in java var i = 0 while ({ i = i + 1 i &lt; 1000 }){ // body of while println(s&#34;i: $i&#34;) } to, until — (1 to 10) will generate a Seq, with the content of (1,2,3,4…10), (0 until 10) will generate a sequence from 0 to 9. With some test, (0 until 1000).map(xxx) appears to be slower than var i=0; while( i &lt; 1000) { i += 1; sth. else}, but if the body of map is pretty heavy, this cost can be ignored.
headOption - Get the head of one list, return None if this list is empty
head - Get the head of one list, throw exception if this list is empty
take &ndash; Get first at most N elements. (from left to right)
scala&gt; List(1,2,3).take(2) res0: List[Int] = List(1, 2) scala&gt; List(1,2).take(3) res1: List[Int] = List(1, 2) drop &ndash; Drop first at most N elements. scala&gt; List(1,2,3).drop(2) res2: List[Int] = List(3) scala&gt; List(1,2).drop(3) res3: List[Int] = List() dropRight will drop elements from right to left.
slice &ndash; Return list in [start-offset, end-offset) scala&gt; List(1,2,3).slice(1,2) res7: List[Int] = List(2) scala&gt; List(1,2,3).slice(2,2) res8: List[Int] = List() val offset = 1 val size = 3 List(1,2,3,4,5).slice(offset, size + offset) If the end-offset is greater than the length of this list, it will not throw exception.
splitAt &ndash; Split this list into two from offset i scala&gt; List(1,2,3).splitAt(1) res10: (List[Int], List[Int]) = (List(1),List(2, 3)) groupBy &ndash; Partitions a list into a map of collections according to a discriminator function scala&gt; List(1,2,3).groupBy(i =&gt; if(i % 2 == 0) &#34;even&#34; else &#34;odd&#34; ) res11: scala.collection.immutable.Map[String,List[Int]] = Map(odd -&gt; List(1, 3), even -&gt; List(2)) partition &ndash; Splits a list into a pair of collections; one with elements that satisfy the predicate, the other with elements that do not, giving the pair of collections (xs filter p, xs.filterNot p). scala&gt; List(1,2,3).partition(_ % 2 == 0) res12: (List[Int], List[Int]) = (List(2),List(1, 3)) grouped &ndash; The grouped method chunks its elements into increments. scala&gt; List(1,2,3,4,5).grouped(2) res13: Iterator[List[Int]] = Iterator(List(1, 2), List(3, 4), List(5)) You can visit this PDF for an official guide.
We also highly recomended to read the book Programming in Scala for more detail instruction.
`}),e.add({id:11,href:"/bigdata-bootcamp/docs/sessions/spark-application/",title:"Spark Application",content:`Spark Application Learning Objectives
Prepare data for machine learning applications. Save/load constructed data to external storage. In this section, we will show how to prepare suitable data for building predictive models to predict heart failure (HF). We will first briefly introduce data types involved. Then we show how to construct training/testing samples from the input data using Spark. Finally we will export data in suitable format for modeling later.
Data Types For many machine learning tasks, such as classification, regression, and clustering, a data point is often represented as a feature vector. Each coordinate of the vector corresponds to a particular feature of the data point.
Feature Vector MLlib, the machine learning module of Spark, supports two types of vectors: dense and sparse. A dense vector is basically a Double array of length equals to the dimension of the vector. If a vector contains only a few non-zero entries, we can then more efficiently represent the vector by a sparse vector with non-zero indices and the corresponding values only. For example, a vector (1.0, 0.0, 3.0) can be represented in dense format as [1.0, 0.0, 3.0] or in sparse format as (3, [0, 2], [1.0, 3.0]), where 3 is the size of the vector.
The base class of a vector is Vector, and there are two implementations: DenseVector and SparseVector. We recommend using the factory methods implemented in Vectors to create vectors.
scala&gt; import org.apache.spark.mllib.linalg.{Vector, Vectors} // Create a dense vector (1.0, 0.0, 3.0). scala&gt; val dv = Vectors.dense(1.0, 0.0, 3.0) // Create a sparse vector (1.0, 0.0, 3.0) by specifying its nonzero entries. scala&gt; val sv = Vectors.sparse(3, Seq((0, 1.0), (2, 3.0))) Labeled Point A labeled point is a vector, either dense or sparse, associated with a label/prediction target. In Spark MLlib, labeled points are used as input to supervised learning algorithms. For example, in binary classification like HF prediction, a label should be either 0 or 1. For multiclass classification, labels should be class indices starting from zero: 0, 1, 2, &hellip;. For regression problem like payment prediction, a label is a real-valued number.
scala&gt; import org.apache.spark.mllib.linalg.Vectors scala&gt; import org.apache.spark.mllib.regression.LabeledPoint // Create a labeled point with label 1 and a dense feature vector. scala&gt; val labeled1 = LabeledPoint(1, Vectors.dense(1.0, 0.0, 3.0)) // Create a labeled point with label 0 and a sparse feature vector. scala&gt; val labeled0 = LabeledPoint(0, Vectors.sparse(3, Seq((0, 1.0), (2, 3.0)))) Feature Construction Overview To apply machine learning algorithms, we need to transform our data into RDD[LabeledPoint]. We will need to consider an one-year prediction window. Specifically, we will only use data one year before HF diagnosis. The figure below depicts relationship between prediction window and target.
We can also specify an observation window, inside which data will be used to construct feature vectors.
High level steps are depicted as the figure below
Our parallelization will be on patient level, i.e. each element in RDD is everything about exactly one patient. Feature and prediction target for each patient is almost independent from the others. Recall that our data file is in the following form:
00013D2EFD8E45D1,DIAG78820,1166,1.0 00013D2EFD8E45D1,DIAGV4501,1166,1.0 00013D2EFD8E45D1,heartfailure,1166,1.0 00013D2EFD8E45D1,DIAG2720,1166,1.0 .... Each line is a 4-tuple (patient-id, event-id, timestamp, value). Suppose now our goal is to predict if a patient will have heart failure. We can use the value associated with the event heartfailure as the label. This value can be either 1.0 (the patient has heart failure) or 0.0 (the patient does not have heart failure). We call a patient with heart failure a positive example or case patient, and a patient without heart failure a negative example or control patient. For example, in the above snippet we can see that patient 00013D2EFD8E45D1 is a positive example. The file case.csv consists of only positive examples, and the file control.csv consists of only negative examples.
We will use the values associated with events other than heartfailure to construct feature vector for each patient. Specifically, the length of the feature vector is the number of distinct event-id&rsquo;s, and each coordinate of the vector stores the aggregated value corresponds to a particular event-id. The values associated with events not shown in the file are assume to be 0. Since each patient typically has only a few hundreds of records (lines) compared to thousands of distinct events, it is more efficient to use SparseVector. Note that each patient can have multiple records with the same event-id. In this case we sum up the values associated with a same event-id as feature value and use event-id as feature name.
1. Load data The file input/case.csv consists of only positive examples, and the file input/control.csv consists of only negative examples. We will load them together. Since the data will be used more than once, we use cache() to prevent reading in the file multiple times.
case class Event(patientId: String, eventId: String, timestamp: Int, value: Double) val rawData = sc.textFile(&#34;input/&#34;). map{line =&gt; val splits = line.split(&#34;,&#34;) new Event(splits(0), splits(1), splits(2).toInt, splits(3).toDouble) } 2. Group patient data One patient&rsquo;s index date, prediction target etc are independent from another patient, so that we can group by patient-id to put everything about one patient together. When we run map operation, Spark will help us parallelize computation on patient level.
// group raw data with patient id and ignore patient id // then we will run parallel on patient lelvel val grpPatients = rawData.groupBy(_.patientId).map(_._2) The groupBy operation can be illustrated with the example below
Please recall that _._2 will return second field of a tuple. In this case it will return the List[event] for a given patient. Finally the grpPatients will be RDD[List[event]]
3. Define target and feature values Now, we can practice our patient level parallelization. For each patient, we first find the prediction target, which is encoded into an event with name heartfailure, then we identify the index date and keep only useful events before the index date for feature construction. In feature construction, we aggregate the event value into features using sum function and use the event name as the feature name.
val patientTargetAndFeatures = grpPatients.map{events =&gt; // find event that encode our prediction target, heart failure val targetEvent = events.find(_.eventId == &#34;heartfailure&#34;).get val target = targetEvent.value // filter out other events to construct features val featureEvents = events.filter(_.eventId != &#34;heartfailure&#34;) // define index date as one year before target // and use events happened one year before heart failure val indexDate = targetEvent.timestamp - 365 val filteredFeatureEvents = featureEvents.filter(_.timestamp &lt;= indexDate) // aggregate events into features val features = filteredFeatureEvents. groupBy(_.eventId). map{case(eventId, grpEvents) =&gt; // user event id as feature name val featureName = eventId // event value sum as feature value val featureValue = grpEvents.map(_.value).sum (featureName, featureValue) } (target, features) } The construction of target is relatively simple, but the process of constructing features is tricky. The example below show what happened in main body of above map operation to illustrate how features were constructed
Our final filteredFeatureEvents should be RDD[(target, Map[feature-name, feature-value])] and we can verify that by the following:
scala&gt; patientTargetAndFeatures.take(1) res0: Array[(Double, scala.collection.immutable.Map[String,Double])] = Array((0.0,Map(DRUG36987241603 -&gt; 60.0, DRUG00378181701 -&gt; 30.0, DRUG11517316909 -&gt; 20.0, DRUG53002055230 -&gt; 200.0, DRUG23490063206 -&gt; 30.0, DRUG61113074382 -&gt; 60.0, DRUG58016093000 -&gt; 60.0, DRUG52604508802 -&gt; 30.0, DRUG58016037228 -&gt; 10.0, DRUG60491080134 -&gt; 30.0, DRUG51079093119 -&gt; 360.0, DRUG00228275711 -&gt; 30.0, DRUG63629290803 -&gt; 120.0, DIAG4011 -&gt; 1.0, DRUG58016075212 -&gt; 90.0, DRUG00378412401 -&gt; 30.0, DRUG63629260701 -&gt; 30.0, DRUG00839619116 -&gt; 30.0, DRUG11390002315 -&gt; 30.0, DRUG58016058050 -&gt; 60.0, DRUG55289082930 -&gt; 60.0, DRUG36987154502 -&gt; 30.0, DRUG00364095301 -&gt; 30.0, DRUG58016021326 -&gt; 180.0, DRUG54868593401 -&gt; 30.0, DRUG58016054035 -&gt; 30.0, DRUG64464000105 -&gt; 30.0, DRUG58016076573 -&gt; 30.0, DRUG00839710006... 4. Feature name to id In the previous step, we computed filteredFeatureEvents as RDD[(target, Map[feature-name, feature-value])]. In order to convert feature-name to some integer id as required by most machine learning modules including MLlib, we will need to collect all unique feauture names and associate them with integer ids.
// assign a unique integer id to feature name val featureMap = patientTargetAndFeatures. // RDD[(target, Map[feature-name, feature-value])] flatMap(_._2.keys). // get all feature names distinct. // remove duplication collect. // collect to driver program zipWithIndex. // assign an integer id toMap // convert to Map[feature-name, feature-id] Here we used an operation named flatMap. Below is an example, and we can think of flatMap as a two step operation, map and flatten. As a result, patientTargetAndFeatures.flatMap(_._2.keys) will give RDD[feature-name].
Next we visualize the steps after flatMap:
Here collect is not depicted but what collect does is to collect data from distributed to centralized storage on the driver. Here we assume the resulting data matrix is not too big. If the data matrix is very big, alternative approach may be required such as join. Note that many the common functions like zipWithIndex have the same name on RDD and on common local data structures like List.
If you get confused about result of certain operations, you can avoid chain of operation calls and instead print out the result of each step. 5. Create final LabeledPoint In this last step, we transform (target, features) for each patient into LabeledPoint. Basically, we just need to translate feature name in features into feautre id and create a feature vector then associate the vector with target.
// broadcast feature map from driver to all workers val scFeatureMap = sc.broadcast(featureMap) val finalSamples = patientTargetAndFeatures.map {case(target, features) =&gt; val numFeature = scFeatureMap.value.size val indexedFeatures = features. toList. // map feature name to id to get List[(feature-id, feature-value)] map{case(featureName, featureValue) =&gt; (scFeatureMap.value(featureName), featureValue)} val featureVector = Vectors.sparse(numFeature, indexedFeatures) val labeledPoint = LabeledPoint(target, featureVector) labeledPoint } Here in above example, we called sc.broadcast. As indicated by its name, this function is used for broadcasting data from driver to workers so that workers will not need to copy on demand and waste bandwidth thus slow down the process. Its usage is very simple, call val broadcasted = sc.broadcast(object) and use broadcasted.value to access original object. Please be aware of the fact that such broadcasted object is read-only.
Save With data readily available as RDD[LabeledPoint], we can save it into a common format accepted by a lot of machine learning modules, the LibSVM/svmlight format, named after LibSVM/svmlight package.
import org.apache.spark.mllib.util.MLUtils MLUtils.saveAsLibSVMFile(finalSamples, &#34;samples&#34;) You can achieve this by
val mapping = featureMap. toList. sortBy(_._2). map(pair =&gt; s&#34;\${pair._1}|\${pair._2}&#34;). // intentionally use special seperator mkString(&#34;\\n&#34;) scala.tools.nsc.io.File(&#34;mapping.txt&#34;).writeAll(mapping) `}),e.add({id:12,href:"/bigdata-bootcamp/docs/sessions/spark-basic/",title:"Spark Basic",content:`Spark Basics Learning Objectives
Invoke command in Spark interactive shell. Be familiar with RDD concept. Know basic RDD operations. Spark Shell Spark can run in several modes, including YARN client/server, Standalone, Mesos and Local. For this training, we will use local mode. Specifically, you can start the Spark interactive shell by invoking the command below in the terminal to run Spark in the local mode with two threads. Then you will see
&gt; spark-shell --master &#34;local[2]&#34; --driver-memory 6G Using Spark&#39;s default log4j profile: org/apache/spark/log4j-defaults.properties ... [messages] ... Spark context available as sc. scala&gt; Here you can set --driver-memory according to your local setting. If your setting of driver memory is larger than the VM memory, don&rsquo;t forget to change the VM memory setting first.
In Spark, we call the main entrance of a Spark program the driver and Spark distribute computation to workers to compute. Here in the interactive shell, the Spark shell program is the driver. In above example we set the memory of driver program to 3GB as in local mode driver and worker are together. A driver program can access Spark through a SparkContext object, which represents a connection to a computing cluster. In the above interactive shell, SparkContext is already created for you as variable sc. You can input sc to see its type.
scala&gt; sc res0: org.apache.spark.SparkContext = org.apache.spark.SparkContext@27896d3b You may find the logging statements that get printed in the shell distracting. You can control the verbosity of the logging. To do this, you can create a file in the conf directory called log4j.properties. The Spark developers already include a template for this file called log4j.properties.template. To make the logging less verbose, make a copy of conf/log4j.properties.template called conf/log4j.properties and find the following line:
log4j.rootCategory=INFO, console Replace INFO with WARN so that only WARN messages and above are shown.
RDD Resilient Distributed Dataset (RDD) is Spark&rsquo;s core abstraction for working with data. An RDD is simply a fault-tolerant distributed collection of elements. You can imagine RDD as a large array but you cannot access elements randomly but you can apply the same operations to all elements in the array easily. In Spark, all the work is expressed as either creating new RDDs, transforming existing RDDs, or calling operations on RDDs to compute results. There are two ways to create RDDs: by distributing a collection of objects (e.g., a list or set), or by referencing a dataset in an external storage system, such as a shared filesystem, HDFS, HBase, or any data source offering a Hadoop InputFormat.
Parallelized Collections For the demo purpose, the simplest way to create an RDD is to take an existing collection (e.g. a Scala Array) in your program and pass it to SparkContext&rsquo;s parallelize() method.
scala&gt; val data = Array(1, 2, 3, 4, 5) data: Array[Int] = Array(1, 2, 3, 4, 5) scala&gt; val distData = sc.parallelize(data) distData: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[0] at parallelize at &lt;console&gt;:23 Once created, the distributed dataset (distData) can be operated in parallel. For example, we can add up the elements by calling distData.reduce((a, b) =&gt; a + b). You will see more operations on RDD later on.
Parallelizing a collection is useful when you are learning Spark. However, this is not encouraged in production since it requires the entire dataset to be in memory of the driver&rsquo;s machine first. Instead, importing data from external datasets should be employed. External Datasets A common way for creating RDDs is loading data from external storage. Below you will learn how to load data from a file system.
Here, we choose to read data from HDFS, which stands for Hadoop Distributed File System. (For more details, you could refer to HDFS Basics.)
When you use HDFS for the first time, it&rsquo;s likely that your home directory in HDFS has not been created yet. In the environment that we provide, there&rsquo;s a special user hdfs who is an HDFS administrator and has the permission to create new home directories.
We need to put the data case.csv into HDFS, and to do this, run the following commands:
&gt; cd /bigdata-bootcamp/data &gt; sudo su - hdfs &gt; hdfs dfs -mkdir -p input &gt; hdfs dfs -chown -R root input &gt; exit &gt; hdfs dfs -put case.csv input What you do above is that first, you switch to the hdfs user. Then, you can create a directory and change ownership of the newly created folder. (Pay attention that here since it is a virtual environment, you don&rsquo;t need to worry about using root user and its permission.) Next, similar to creating local directory via linux command mkdir, you creating a folder named input in HDFS. In hdfs dfs -mkdir, hdfs is the HDFS utility program, dfs is the subcommand to handle basic HDFS operations, -mkdir means you want to create a directory and the directory name is specified as input. These commands actually create the input directory in your home directory in HDFS. Of course, you can create it to other place with absolute or relative path. Finally, after exiting HDFS, you copy data from local file system to HDFS using -put.
scala&gt; val lines = sc.textFile(&#34;input/case.csv&#34;) lines: org.apache.spark.rdd.RDD[String] = README.md MapPartitionsRDD[1] at textFile at &lt;console&gt;:21 Here in the above example, each line of the original file will become an element in the lines RDD.
Reading data from a file system, Spark relies on the HDFS library. In above example we assume HDFS is well configured through environmental variables or configuration files so that data is ready in HDFS. RDD Operations RDDs offer two types of operations: transformations and actions:
Transformations are operations on RDDs that return a new RDD, such as map() and filter(). Actions are operations that return a result to the driver program or write it to storage, such as first() and count(). Spark treats transformations and actions very differently, so understanding which type of operation you are performing is very important. You can check whether a function is a transformation or an action by looking at its return type: transformations return RDDs, whereas actions return some other data type.
All transformations in Spark are lazy, in that they do not compute the results right away. Instead, they just remember the operations applied to some base dataset (e.g. an Array or a file). The transformations are only computed when an action requires a result to be returned to the driver program. Therefore, the above command of reading in a file has not actually been executed yet. We can force the evaluation of RDDs by calling any actions.
Let&rsquo;s go through some common RDD operations using the healthcare dataset. Recall that in the file case.csv, each line is a 4-field tuple (patient-id, event-id, timestamp, value).
Count In order to know how large is our raw event sequence data, we can count the number of lines in the input file using count operation, i.e.
scala&gt; lines.count() res1: Long = 14046 Clearly, count is an action.
Take You may wonder what the loaded data looks like, you can take a peek at the data. The take(k) will return the first k elements in the RDD. Spark also provides collect() which brings all the elements in the RDD back to the driver program. Note that collect() should only be used when the data is small. Both take and collect are actions.
scala&gt; lines.take(5) res2: Array[String] = Array(00013D2EFD8E45D1,DIAG78820,1166,1.0, 00013D2EFD8E45D1,DIAGV4501,1166,1.0, 00013D2EFD8E45D1,heartfailure,1166,1.0, 00013D2EFD8E45D1,DIAG2720,1166,1.0, 00013D2EFD8E45D1,DIAG4019,1166,1.0) We got the first 5 records in this RDD. However, this is hard to read due to a poor format. We can make it more readable by traversing the array to print each record on its own line.
scala&gt; lines.take(5).foreach(println) 00013D2EFD8E45D1,DIAG78820,1166,1.0 00013D2EFD8E45D1,DIAGV4501,1166,1.0 00013D2EFD8E45D1,heartfailure,1166,1.0 00013D2EFD8E45D1,DIAG2720,1166,1.0 00013D2EFD8E45D1,DIAG4019,1166,1.0 Note that in above 3 code block examples, the RDD lines has been computed (i.e. read in from file) 3 times. We can prevent this by calling lines.cache(), which will cache the RDD in memory to avoid reloading.
scala&gt; lines.take(5).map(_.split(&#34;,&#34;)).map(_(1)).foreach(println) Map The map operation in Spark is similar to that of Hadoop. It&rsquo;s a transformation that transforms each item in the RDD into a new item by applying the provided function. Notice this map will map exactly one element from source to target. For example, suppose we are only interested in knowing IDs of patients, we use map like
scala&gt; lines.map(line =&gt; line.split(&#34;,&#34;)(0)) It is also possible to write a more complex, multiple-lines map function. In this case, curly braces should be used in place of parentheses. For example, we can get both patient-id and event-id as a tuple at the same time.
scala&gt; lines.map{line =&gt; val s = line.split(&#34;,&#34;) (s(0), s(1)) } Filter As indicated by its name, filter can transform an RDD to another RDD by keeping only elements that satisfy the filtering condition. For example, we want to count the number of events collected for a particular patient to verify amount of the data from that patient. We can use a filter function.
scala&gt; lines.filter(line =&gt; line.contains(&#34;00013D2EFD8E45D1&#34;)).count() res4: Long = 200 Distinct distinct is a transformation that transforms a RDD to another by eliminating duplications. We can use that to count the number of distinct patients. In order to do this, we first extract the patient ID from each line. We use the map() function as described above. In this example, we transform each line into the corresponding patient ID by extracting only the first column. We then eliminate duplicate IDs by the distinct() function.
scala&gt; lines.map(line =&gt; line.split(&#34;,&#34;)(0)).distinct().count() res5: Long = 100 Group Sometimes, you will need to group the input events according to patient-id to put everything about each patient together. For example, in order to extract index date for predictive modeling, you may first group input data by patient then handle each patient seperately in parallel. We can see each element in RDD is a (Key, Value) pair (patient-id, iterable[event]).
&gt; val patientIdEventPair = lines.map{line =&gt; val patientId = line.split(&#34;,&#34;)(0) (patientId, line) } &gt; val groupedPatientData = patientIdEventPair.groupByKey &gt; groupedPatientData.take(1) res1: Array[(String, Iterable[String])] = Array((0102353632C5E0D0,CompactBuffer(0102353632C5E0D0,DIAG29181,562,1.0, 0102353632C5E0D0,DIAG29212,562,1.0, 0102353632C5E0D0,DIAG34590,562,1.0, 0102353632C5E0D0,DIAG30000,562,1.0, 0102353632C5E0D0,DIAG2920,562,1.0, 0102353632C5E0D0,DIAG412,562,1.0, 0102353632C5E0D0,DIAG28800,562,1.0, 0102353632C5E0D0,DIAG30391,562,1.0, 0102353632C5E0D0,DIAGRG894,562,1.0, 0102353632C5E0D0,PAYMENT,562,6000.0, 0102353632C5E0D0,DIAG5781,570,1.0, 0102353632C5E0D0,DIAG53010,570,1.0, 0102353632C5E0D0,DIAGE8490,570,1.0, 0102353632C5E0D0,DIAG27651,570,1.0, 0102353632C5E0D0,DIAG78559,570,1.0, 0102353632C5E0D0,DIAG56210,570,1.0, 0102353632C5E0D0,DIAG5856,570,1.0, 0102353632C5E0D0,heartfailure,570,1.0, 0102353632C5E0D0,DIAG5070,570,1.0, 0102353632C5E0D0,DIAGRG346,570,1.0,... .... Reduce By Key reduceByKey transforms an RDD[(K, V)] into RDD[(K, List[V])] (like what groupByKey does) and then apply reduce function on List[V] to get final output RDD[(K, V)]. Please be careful that we intentionally denote V as return type of reduce which should be same as input type of the list element. Suppose now we want to calculate the total payment by each patient. A payment record in the dataset is in the form of (patient-id, PAYMENT, timestamp, value).
val payment_events = lines.filter(line =&gt; line.contains(&#34;PAYMENT&#34;)) val payments = payment_events.map{ x =&gt; val s = x.split(&#34;,&#34;) (s(0), s(3).toFloat) } val paymentPerPatient = payments.reduceByKey(_+_) The payment_events RDD returned by filter contains those records associated with payment. Each item is then transformed to a key-value pair (patient-id, payment) with map. Because each patient can have multiple payments, we need to use reduceByKey to sum up the payments for each patient. Here in this example, patient-id will be served as the key, and payment will be the value to sum up for each patient. The figure below shows the process of reduceByKey in our example
Sort We can then find the top-3 patients with the highest payment by using sortBy first.
scala&gt; paymentPerPatient.sortBy(_._2, false).take(3).foreach(println) and output is
(0085B4F55FFA358D,139880.0) (019E4729585EF3DD,108980.0) (01AC552BE839AB2B,108530.0) Again in sortBy we use the _ placeholder, so that _._2 is an anonymous function that returns the second element of a tuple, which is the total payment a patient. The second parameter of sortBy controls the order of sorting. In above example, false means decreasing order.
scala&gt; val maxPaymentPerPatient = payments.reduceByKey(math.max) Here, reduceByKey(math.max) is the simplified expression of reduceByKey(math.max(_,_)) or reduceByKey((a,b) =&gt; math.max(a,b)). math.max is a function in scala that turns the larger one of two parameters.
&lt;ExerciseComponent question=&ldquo;Count the number of records for each drug (event-id starts with &ldquo;DRUG&rdquo;)&rdquo; answer=&quot;&quot;&gt;
scala&gt; val drugFrequency = lines.filter(_.contains(&#34;DRUG&#34;)). map{ x =&gt; val s = x.split(&#34;,&#34;) (s(1), 1) }.reduceByKey(_+_) Statistics Now we have total payment information of patients, we can run some basic statistics. For RDD consists of numeric values, Spark provides some useful statistical primitives.
scala&gt; val payment_values = paymentPerPatient.map(payment =&gt; payment._2).cache() scala&gt; payment_values.max() res6: Float = 139880.0 scala&gt; payment_values.min() res7: Float = 3910.0 scala&gt; payment_values.sum() res8: Double = 2842480.0 scala&gt; payment_values.mean() res9: Double = 28424.8 scala&gt; payment_values.stdev() res10: Double = 26337.091771112468 Set Operation RDDs support many of the set operations, such as union and intersection, even when the RDDs themselves are not properly sets. For example, we can combine the two files by the union function. Please notice that union here is not strictly identical to union operation in mathematics as Spark will not remove duplications.
scala&gt; val linesControl = sc.textFile(&#34;input/control.csv&#34;) scala&gt; lines.union(linesControl).count() res11: Long = 31144 Here, a more straightforward way is to use directory name to read in multiple files of that directory into a single RDD.
scala&gt; val lines = sc.textFile(&#34;input/&#34;) scala&gt; val drugCase = sc.textFile(&#34;input/case.csv&#34;). filter(_.contains(&#34;DRUG&#34;)). map(_.split(&#34;,&#34;)(1)). distinct() scala&gt; val drugControl = sc.textFile(&#34;input/control.csv&#34;). filter(_.contains(&#34;DRUG&#34;)). map(_.split(&#34;,&#34;)(1)). distinct() scala&gt; drugCase.intersection(drugControl).count() res: Long = 396 Datasets This section is work in progress Dataset is added from Spark 1.6+ A Dataset is a new interface added from Spark 1.6 that tries to provide the benefits of RDDs (strong typing, ability to use powerful lambda functions) with the benefits of Spark SQL’s optimized execution engine.
A Dataset has a concrete type of a Scala primitive type (Integer, Long, Boolean, etc) or a subclass of a Product - a case class. The case class is preferred for Spark because it handles the serialization code for you thus allowing Spark to shuffle data between workers. This can additional be handled implementing Externalizable which is a much more efficient mechanism to handle serialization, or by using a compact serializer like Kryos.
Additionally, Spark no longer uses SparkContext directly but prefers the use of a SparkSession that encapsulates a SparkContext and a SqlContext. The SparkSession is a member of the sql package.
There is a wealth of great documentation on the Spark development site.
Creation of Datasets Datasets can be created explicitly or loaded form a source (e.g. file, stream, parquet, etc).
case class Person(firstName: String, lastName:String) // wire-in spark implicits import spark.implicits._ case class Person(firstName: String, lastName: String) val ds = Seq(Person(&#34;Daniel&#34;, &#34;Williams&#34;)).toDS() // here you can perform operations that are deferred until an action is invoked. // creates a anonymous lambda that looks at the // firstName of the Dataset[Person] type and invokes a collect // to pull data back to the driver as an Array[Person] // the foreach then will invoke a println on each Person // instance and implicit apply the toString operation that is // held in the Product trait ds.filter(_.firstName == &#34;Daniel&#34;).collect().foreach(println) Spark SQL, DataFrames and Datasets Guide Further Reading For the complete list of RDD operations, please see the Spark Programming Guide.
`}),e.add({id:13,href:"/bigdata-bootcamp/docs/sessions/spark-graphx/",title:"Spark Graphx",content:`Spark GraphX Learning Objectives
Understand composition of a graph in Spark GraphX. Being able to create a graph. Being able to use the built-in graph algorithm. In this section we begin by creating a graph with patient and diagnostic codes. Later we will show how to run graph algorithms on the the graph you will create.
Basic concept Spark GraphX abstracts a graph as a concept named Property Graph, which means that each edge and vertex is associated with some properties. The Graph class has the following definition
class Graph[VD, ED] { val vertices: VertexRDD[VD] val edges: EdgeRDD[ED] } Where VD and ED define property types of each vertex and edge respectively. We can regard VertexRDD[VD] as RDD of (VertexID, VD) tuple and EdgeRDD[ED] as RDD of (VertexID, VertexID, ED).
Graph construction Let&rsquo;s create a graph of patients and diagnostic codes. For each patient we can assign its patient id as vertex property, and for each diagnostic code, we will use the code as vertex property. For the edge between patient and diagnostic code, we will use number of times the patient is diagnosed with given disease as edge property.
Define class Let&rsquo;s first define necessary data structure and import
import org.apache.spark.SparkContext._ import org.apache.spark.graphx._ import org.apache.spark.rdd.RDD abstract class VertexProperty extends Serializable case class PatientProperty(patientId: String) extends VertexProperty case class DiagnosticProperty(icd9code: String) extends VertexProperty case class PatientEvent(patientId: String, eventName: String, date: Int, value: Double) Load raw data Load patient event data and filter out diagnostic related events only
val allEvents = sc.textFile(&#34;input/&#34;). map(_.split(&#34;,&#34;)). map(splits =&gt; PatientEvent(splits(0), splits(1), splits(2).toInt, splits(3).toDouble)) // get and cache diagnosticEvents as we will reuse val diagnosticEvents = allEvents. filter(_.eventName.startsWith(&#34;DIAG&#34;)).cache() Create vertex Patient vertex Let&rsquo;s create patient vertex
// create patient vertex val patientVertexIdRDD = diagnosticEvents. map(_.patientId). distinct. // get distinct patient ids zipWithIndex // assign an index as vertex id val patient2VertexId = patientVertexIdRDD.collect.toMap val patientVertex = patientVertexIdRDD. map{case(patientId, index) =&gt; (index, PatientProperty(patientId))}. asInstanceOf[RDD[(VertexId, VertexProperty)]] In order to use the newly created vertex id, we finally collect all the patient to VertrexID mapping.
Theoretically, collecting RDD to driver is not an efficient practice. One can obtain uniqueness of ID by calculating ID directly with a Hash. Diagnostic code vertex Similar to patient vertex, we can create diagnostic code vertex with
// create diagnostic code vertex val startIndex = patient2VertexId.size val diagnosticVertexIdRDD = diagnosticEvents. map(_.eventName). distinct. zipWithIndex. map{case(icd9code, zeroBasedIndex) =&gt; (icd9code, zeroBasedIndex + startIndex)} // make sure no conflict with patient vertex id val diagnostic2VertexId = diagnosticVertexIdRDD.collect.toMap val diagnosticVertex = diagnosticVertexIdRDD. map{case(icd9code, index) =&gt; (index, DiagnosticProperty(icd9code))}. asInstanceOf[RDD[(VertexId, VertexProperty)]] Here we assign vertex id by adding the result of zipWithIndex with an offset obtained from previous patient vertex to avoid ID confliction between patient and diagnostic code.
Create edge In order to create edge, we will need to know vertext id of vertices we just created.
val bcPatient2VertexId = sc.broadcast(patient2VertexId) val bcDiagnostic2VertexId = sc.broadcast(diagnostic2VertexId) val edges = diagnosticEvents. map(event =&gt; ((event.patientId, event.eventName), 1)). reduceByKey(_ + _). map{case((patientId, icd9code), count) =&gt; (patientId, icd9code, count)}. map{case(patientId, icd9code, count) =&gt; Edge( bcPatient2VertexId.value(patientId), // src id bcDiagnostic2VertexId.value(icd9code), // target id count // edge property )} We first broadcast patient and diagnostic code to vertext id mapping. Broadcast can avoid unnecessary copy in distributed setting thus will be more effecient. Then we count occurrence of (patient-id, icd-9-code) pairs with map and reduceByKey, finally we translate them to proper VertexID.
Assemble vertex and edge We will need to put vertices and edges together to create the graph
val vertices = sc.union(patientVertex, diagnosticVertex) val graph = Graph(vertices, edges) Graph operation Given the graph we created, we can run some basic graph operations.
Connected components Connected component can help find disconnected subgraphs. GraphX provides the API to get connected components as below
val connectedComponents = graph.connectedComponents The return result is a graph and assigned components of original graph is stored as VertexProperty. For example
scala&gt; connectedComponents.vertices.take(5) Array[(org.apache.spark.graphx.VertexId, org.apache.spark.graphx.VertexId)] = Array((2556,0), (1260,0), (1410,0), (324,0), (180,0)) The first element of the tuple is VertexID identical to original graph. The second element in the tuple is connected component represented by the lowest-numbered VertexID in that component. In above example, five vertices belong to same component.
We can easily get number of connected components using operations on RDD as below.
scala&gt; connectedComponents.vertices.map(_._2).distinct.collect Array[org.apache.spark.graphx.VertexId] = Array(0, 169, 239) Degree The property graph abstraction of GraphX is a directed graph. It provides computation of in-dgree, out-degree and total degree. For example, we can get degrees as
val inDegrees = graph.inDegrees val outDegrees = graph.outDegrees val totalDegrees = graph.degrees PageRank GraphX also provides implementation of the famous PageRank algorithm, which can compute the &lsquo;importance&rsquo; of a vertex. The graph we generated above is a bipartite graph and not suitable for PageRank. To gve an example of PageRank, we randomly generate a graph and run fixed iteration of PageRank algorithm on it.
import org.apache.spark.graphx.util.GraphGenerators val randomGraph:Graph[Long, Int] = GraphGenerators.logNormalGraph(sc, numVertices = 100) val pagerank = randomGraph.staticPageRank(20) Or, we can run PageRank until converge with tolerance as 0.01 using randomGraph.pageRank(0.01)
Application Next, we show some how we can ultilize the graph operations to solve some practical problems in the healthcare domain.
Explore comorbidities Comorbidity is additional disorders co-occuring with primary disease. We know all the case patients have heart failure, we can explore possible comorbidities as below (see comments for more explaination)
// get all the case patients val casePatients = allEvents. filter(event =&gt; event.eventName == &#34;heartfailure&#34; &amp;&amp; event.value == 1.0). map(_.patientId). collect. toSet // broadcast val scCasePatients = sc.broadcast(casePatients) //filter the graph with subGraph operation val filteredGraph = graph.subgraph(vpred = {case(id, attr) =&gt; val isPatient = attr.isInstanceOf[PatientProperty] val patient = if(isPatient) attr.asInstanceOf[PatientProperty] else null // return true iff. isn&#39;t patient or is case patient !isPatient || (scCasePatients.value contains patient.patientId) }) //calculate indegrees and get top vertices val top5ComorbidityVertices = filteredGraph.inDegrees. takeOrdered(5)(scala.Ordering.by(-_._2)) We have
top5ComorbidityVertices: Array[(org.apache.spark.graphx.VertexId, Int)] = Array((3129,86), (335,63), (857,58), (2048,49), (669,48)) And we can check the vertex of index 3129 in original graph is
scala&gt; graph.vertices.filter(_._1 == 3129).collect Array[(org.apache.spark.graphx.VertexId, VertexProperty)] = Array((3129,DiagnosticProperty(DIAG4019))) The 4019 code correponds to Hypertension, which is reasonable.
Similar patients Given a patient diagnostic graph, we can also find similar patients. One of the most straightforward approach is shortest path on the graph.
val sssp = graph. mapVertices((id, _) =&gt; if (id == 0L) 0.0 else Double.PositiveInfinity). pregel(Double.PositiveInfinity)( (id, dist, newDist) =&gt; math.min(dist, newDist), // Vertex Program triplet =&gt; { // Send Message var msg: Iterator[(org.apache.spark.graphx.VertexId, Double)] = Iterator.empty if (triplet.srcAttr + 1 &lt; triplet.dstAttr) { msg = msg ++ Iterator((triplet.dstId, triplet.srcAttr + 1)) } if (triplet.dstAttr + 1 &lt; triplet.srcAttr) { msg = msg ++ Iterator((triplet.srcId, triplet.dstAttr + 1)) } println(msg) msg }, (a,b) =&gt; math.min(a,b) // Merge Message ) // get top 5 most similar sssp.vertices.filter(_._2 &lt; Double.PositiveInfinity). filter(_._1 &lt; 300). takeOrdered(5)(scala.Ordering.by(-_._2)) `}),e.add({id:14,href:"/bigdata-bootcamp/docs/sessions/spark-mllib/",title:"Spark Mllib",content:`Spark MLlib and Scikit-learn Learning Objectives
Understand input to MLlib. Learn to run basic classification algorithms. Learn to export/load trained models. Develop models using python machine learning module. In this section, you will learn how to build a heart failure (HF) predictive model. You should have finished previous Spark Application section. You will first learn how to train a model using Spark MLlib and save it. Next, you will learn how to achieve same goal using Python Scikit-learn machine learning module for verification purpose.
MLlib You will first load data and compute some high-level summary statistics, then train a classifier to predict heart failure.
Load Samples Loading data from previously saved data can be achieved by
import org.apache.spark.mllib.util.MLUtils val data = MLUtils.loadLibSVMFile(sc, &#34;samples&#34;) Basic Statistics Spark MLlib provides various functions to compute summary statistics that are useful when doing machine learning and data analysis tasks.
import org.apache.spark.mllib.stat.{MultivariateStatisticalSummary, Statistics} // colStats() calculates the column statistics for RDD[Vector] // we need to extract only the features part of each LabeledPoint: // RDD[LabeledPoint] =&gt; RDD[Vector] val summary = Statistics.colStats(data.map(_.features)) // summary.mean: a dense vector containing the mean value for each feature (column) // the mean of the first feature is 0.3 summary.mean(0) // the variance of the first feature summary.variance(0) // the number of non-zero values of the first feature summary.numNonzeros(0) Split data In a typical machine learning problem, we need to split data into training (60%) and testing (40%) set.
val splits = data.randomSplit(Array(0.6, 0.4), seed = 15L) val train = splits(0).cache() val test = splits(1).cache() Train classifier Let&rsquo;s train a linear SVM model using Stochastic Gradient Descent (SGD) on the training set to predict heart failure
import org.apache.spark.mllib.classification.SVMWithSGD val numIterations = 100 val model = SVMWithSGD.train(train, numIterations) Testing For each sample in the testing set, output a (prediction, label) pair, and calculate the prediction accuracy. We use the broadcast mechanism to avoid unnecessary data copy.
val scModel = sc.broadcast(model) val predictionAndLabel = test.map(x =&gt; (scModel.value.predict(x.features), x.label)) val accuracy = predictionAndLabel.filter(x =&gt; x._1 == x._2).count / test.count.toFloat println(&#34;testing Accuracy = &#34; + accuracy) Save &amp; load model In real world setting, you may need to save the trained model. You can achieve that by directly serialize you model object using java ObjectOutputStream and save
import java.io.{FileOutputStream, ObjectOutputStream, ObjectInputStream, FileInputStream} // save model val oos = new ObjectOutputStream(new FileOutputStream(&#34;model&#34;)) oos.writeObject(model) oos.close() // load model from disk val ois = new ObjectInputStream(new FileInputStream(&#34;model&#34;)) val loadedModel = ois.readObject().asInstanceOf[org.apache.spark.mllib.classification.SVMModel] ois.close() Scikit-learn If typical data set is often small enough after feature construction described in previous Spark Application section, you may consider running machine learning predictive model training and testing using your familiar tools like scikit-learn in Python or some R packages. Here we show how to do that in Scikit-learn, a Python machine learning library.
Fetch data In order to work with Scikit-learn, you will need to take data out of HDFS into a local file system. We can get the samples folder from your home directory in HDFS and merge content into one single file with the command below
hdfs dfs -getmerge samples patients.svmlight Move on with Python In later steps, you will use python interactive shell. To open a python interactive shell, just type python in bash. You will get prompt similar to the sample below
Since the default library does not have sklearn, you need to install it by
pip install sklearn [hang@bootcamp1 ~]$ python Python 2.7.10 |Continuum Analytics, Inc.| (default, Oct 19 2015, 18:04:42) [GCC 4.4.7 20120313 (Red Hat 4.4.7-1)] on linux2 Type &#34;help&#34;, &#34;copyright&#34;, &#34;credits&#34; or &#34;license&#34; for more information. Anaconda is brought to you by Continuum Analytics. Please check out: http://continuum.io/thanks and https://anaconda.org &gt;&gt;&gt; which show version and distribution of the python installation you are using. Here we pre-installed Anaconda
Load and split data Now we can load data and split it into training and testing set in similar way as the MLlib approach.
from sklearn.model_selection import train_test_split from sklearn.datasets import load_svmlight_file X, y = load_svmlight_file(&#34;patients.svmlight&#34;) X = X.toarray() # make it dense X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=41) Train classifier Let&rsquo;s train a linear SVM model again on the training set to predict heart failure
from sklearn.svm import LinearSVC model = LinearSVC(C=1.0, random_state=42, max_iter=10000) model.fit(X_train, y_train) Testing We can get prediction accuracy and AUC on testing set as
from sklearn.metrics import roc_auc_score accuracy = model.score(X_test, y_test) y_score = model.decision_function(X_test) auc = roc_auc_score(y_test, y_score) print(&#34;accuracy = &#34;, accuracy, &#34;AUC = &#34;, auc) Save &amp; load model We can save and load the trained model via pickle serialization module in Python like
import pickle with open(&#39;pysvcmodel.pkl&#39;, &#39;wb&#39;) as f: pickle.dump(model, f) with open(&#39;pysvcmodel.pkl&#39;, &#39;rb&#39;) as f: loaded_model = pickle.load(f) Sparsity and predictive features Since we have limited training data but a large number of features, we may consider using L1 penalty on model to regularize parameters.
from sklearn.preprocessing import MinMaxScaler scaler = MinMaxScaler() X_train = scaler.fit_transform(X_train) X_test = scaler.transform(X_test) l1_model = LinearSVC(C=1.0, random_state=42, dual=False, penalty=&#39;l1&#39;) l1_model.fit(X_train, y_train) accuracy = l1_model.score(X_test, y_test) y_score = l1_model.decision_function(X_test) auc = roc_auc_score(y_test, y_score) print(&#34;for sparse model, accuracy = &#34;, accuracy, &#34;AUC = &#34;, auc) Before fitting a model, we scaled the data to make sure weights of features are comparable. With the sparse model we get from previous example, we can actually identify predictive features according to their coefficients. Here we assume you did the last exercise of previous section about Spark Application. If not, please do that first.
import numpy as np ## loading mapping mapping = [] with open(&#39;mapping.txt&#39;) as f: for line in f.readlines(): splits = line.split(&#39;|&#39;) # feature-name | feature-index mapping.append(splits[0]) ## get last 10 - the largest 10 indices top_10 =np.argsort(l1_model.coef_[0])[-10:] for index, fid in enumerate(top_10[::-1]): #read in reverse order print(&#34;%d: feature [%s] with coef %.3f&#34; % (index, mapping[fid], l1_model.coef_[0][fid]) ) `}),e.add({id:15,href:"/bigdata-bootcamp/docs/sessions/spark-sql/",title:"Spark SQL",content:`Spark Sql Learning Objectives
Load data into Spark SQL as DataFrame. Manipulate data with built-in functions. Define a User Defined Function (UDF). Overview Recent versions of Spark released the programming abstraction named DataFrame, which can be regarded as a table in a relational database. DataFrame is stored in a distributed manner so that different rows may locate on different machines. On DataFrame you can write sql queries, manipulate columns programatically with API etc.
Loading data Spark provides an API to load data from JSON, Parquet, Hive table etc. You can refer to the official Spark SQL programming guide for those formats. Here we show how to load csv files. And we will use the spark-csv module by Databricks.
Start the Spark shell in local mode with the command below to add extra dependencies which are needed to complete this training.
% spark-shell --master &#34;local[2]&#34; --driver-memory 3G --packages com.databricks:spark-csv_2.11:1.5.0 [logs] Spark context available as sc. 15/05/04 13:12:57 INFO SparkILoop: Created sql context (with Hive support).. SQL context available as sqlContext. scala&gt; Spark 2.0+ has built-in csv library now. This parameter is not required any more, and it is only used as a sample. You may want to hide the log messages from spark. You can achieve that by
import org.apache.log4j.Logger import org.apache.log4j.Level Logger.getRootLogger.setLevel(Level.ERROR) Now load data into the shell.
scala&gt; val sqlContext = spark.sqlContext sqlContext: org.apache.spark.sql.SQLContext = org.apache.spark.sql.SQLContext@5cef5fc9 scala&gt; val patientEvents = sqlContext.load(&#34;input/&#34;, &#34;com.databricks.spark.csv&#34;). toDF(&#34;patientId&#34;, &#34;eventId&#34;, &#34;date&#34;, &#34;rawvalue&#34;). withColumn(&#34;value&#34;, &#39;rawvalue.cast(&#34;Double&#34;)) patientEvents: org.apache.spark.sql.DataFrame = [patientId: string, eventId: string, date: string, rawvalue: string, value: double] The first parameter is path to the data (in HDFS), and second is a class name, the specific adapter required to load a CSV file. Here we specified a directory name instead of a specific file name so that all files in that directory will be read and combined into one file. Next we call toDF to rename the columns in the CSV file with meaningful names. Finally, we add one more column that has double type of value instead of string which we will use ourselves for the rest of this material.
Manipulating data There are two methods to work with the DataFrame, either using SQL or using domain specific language (DSL).
SQL Writing SQL is straightforward assuming you have experiences with relational databases.
scala&gt; patientEvents.registerTempTable(&#34;events&#34;) scala&gt; sqlContext.sql(&#34;select patientId, eventId, count(*) count from events where eventId like &#39;DIAG%&#39; group by patientId, eventId order by count desc&#34;).collect res5: Array[org.apache.spark.sql.Row] = Array(...) Here the patientEvents DataFrame is registered as a table in sql context so that we could run sql commands. Next line is a standard sql command with where, group by and order by statements.
DSL Next, we show how to manipulate data with DSL, the same result of previous SQL command can be achieved by:
scala&gt; patientEvents.filter($&#34;eventId&#34;.startsWith(&#34;DIAG&#34;)).groupBy(&#34;patientId&#34;, &#34;eventId&#34;).count.orderBy($&#34;count&#34;.desc).show patientId eventId count 00291F39917544B1 DIAG28521 16 00291F39917544B1 DIAG58881 16 00291F39917544B1 DIAG2809 13 00824B6D595BAFB8 DIAG4019 11 0085B4F55FFA358D DIAG28521 9 6A8F2B98C1F6F5DA DIAG58881 8 019E4729585EF3DD DIAG4019 8 0124E58C3460D3F8 DIAG4019 8 2D5D3D5F03C8C176 DIAG4019 8 01A999551906C787 DIAG4019 7 ... For complete DSL functions, see DataFrame class API.
Saving data Spark SQL provides a convenient way to save data in a different format just like loading data. For example you can write
scala&gt; patientEvents. filter($&#34;eventId&#34;.startsWith(&#34;DIAG&#34;)). groupBy(&#34;patientId&#34;, &#34;eventId&#34;). count. orderBy($&#34;count&#34;.desc). write.json(&#34;aggregated.json&#34;) to save your transformed data in json format or
scala&gt; patientEvents. filter($&#34;eventId&#34;.startsWith(&#34;DIAG&#34;)). groupBy(&#34;patientId&#34;, &#34;eventId&#34;).count. orderBy($&#34;count&#34;.desc). write.format(&#34;com.databricks.spark.csv&#34;).save(&#34;aggregated.csv&#34;) to save in csv format.
UDF In many cases the built-in function of SQL like count, max is not enough, you can extend it with your own functions. For example, you want to find the number of different event types with the following UDF.
Define Define and register an UDF
scala&gt; sqlContext.udf.register(&#34;getEventType&#34;, (s: String) =&gt; s match { case diagnostics if diagnostics.startsWith(&#34;DIAG&#34;) =&gt; &#34;diagnostics&#34; case &#34;PAYMENT&#34; =&gt; &#34;payment&#34; case drug if drug.startsWith(&#34;DRUG&#34;) =&gt; &#34;drug&#34; case procedure if procedure.startsWith(&#34;PROC&#34;) =&gt; &#34;procedure&#34; case &#34;heartfailure&#34; =&gt; &#34;heart failure&#34; case _ =&gt; &#34;unknown&#34; }) Use Write sql and call your UDF
scala&gt; sqlContext.sql(&#34;select getEventType(eventId) type, count(*) count from events group by getEventType(eventId) order by count desc&#34;).show type count drug 16251 diagnostics 10820 payment 3259 procedure 514 heart failure 300 SQL scala&gt; sqlContext.sql(&#34;select patientId, sum(value) as payment from events where eventId = &#39;PAYMENT&#39; group by patientId order by payment desc limit 10&#34;).show patientId payment 0085B4F55FFA358D 139880.0 019E4729585EF3DD 108980.0 01AC552BE839AB2B 108530.0 0103899F68F866F0 101710.0 00291F39917544B1 99270.0 01A999551906C787 84730.0 01BE015FAF3D32D1 83290.0 002AB71D3224BE66 79850.0 51A115C3BD10C42B 76110.0 01546ADB01630C6C 68190.0 DSL scala&gt; patientEvents.filter(patientEvents(&#34;eventId&#34;) === &#34;PAYMENT&#34;).groupBy(&#34;patientId&#34;).agg(&#34;value&#34; -&gt; &#34;sum&#34;).withColumnRenamed(&#34;sum(value)&#34;, &#34;payment&#34;).orderBy($&#34;payment&#34;.desc).show(10) patientId payment 0085B4F55FFA358D 139880.0 019E4729585EF3DD 108980.0 01AC552BE839AB2B 108530.0 0103899F68F866F0 101710.0 00291F39917544B1 99270.0 01A999551906C787 84730.0 01BE015FAF3D32D1 83290.0 002AB71D3224BE66 79850.0 51A115C3BD10C42B 76110.0 01546ADB01630C6C 68190.0 `}),e.add({id:16,href:"/bigdata-bootcamp/docs/sessions/zeppelin-intro/",title:"Zeppelin Intro",content:`How to start Zeppelin Learning Objectives
Learn how to work with Zeppelin Notebook. You can skip this section, if you use your locally installed Zeppelin 1. Run provided Docker image Please prepare your docker environment and refer to this section to start your zeppelin service.
Shared Folder You can use shared folder between your local OS and the virtual environment on Docker. This shared folder can be used to get data from your local and/or to save data without losing it after you exit/destroy your virtual environment. Use -v option to make shared folder from an existing local folder and a folder in virtual environment:
-v &lt;local_folder:vm_folder&gt;
You should use absolute path for vm_folder, but it does not need to be an existing folder. For example, if want to use ~/Data/ in my local OS as shared folder connected with /sample_data/ in VM, I can start a container as following:
docker run -it --privileged=true \\ --cap-add=SYS_ADMIN \\ -m 8192m -h bootcamp1.docker \\ --name bigbox -p 2222:22 -p 9530:9530 -p 8888:8888\\ -v /path/to/Data/:/sample_data/ \\ sunlab/bigbox:latest \\ /bin/bash 2. Start Zeppelin service and create HDFS folder If you have not started Zeppelin service,
/scripts/start-zeppelin.sh We need to create a HDFS folder for the user zeppelin as:
sudo su - hdfs # switch to user &#39;hdfs&#39; hdfs dfs -mkdir -p /user/zeppelin # create folder in hdfs hdfs dfs -chown zeppelin /user/zeppelin # change the folder owner exit You can check whether it has been created or not by using:
hdfs dfs -ls /user/ 3. Open Zeppelin Notebook in your browser Once you have started Zeppelin service and have created HDFS folder for Zeppelin, you can access Zeppelin Notebook by using your local web browser.
Open your web browser, and type in the address: host-ip:port-for-zeppelin For example, 192.168.99.100:9530 since the IP address assigned to my Docker container is 192.168.99.100 as it is shown above, and the port number assigned to Zeppelin service is 9530 as default in our Docker image.
Once you navigate to that IP address with the port number, you will see the front page of Zeppelin like: Let&rsquo;s move to do a simple tutorial in the next section.
`}),e.add({id:17,href:"/bigdata-bootcamp/docs/sessions/zeppelin-tutorial/",title:"Zeppelin Tutorial",content:`Zeppelin Basic Tutorial Learning Objectives
Try to follow the official tutorial of Zeppelin Notebook step-by-step. 1. Create a new Notebook Click on &lsquo;Create new note&rsquo;, and give a name, click on &lsquo;Create Note&rsquo;: Then, you will see a new blank note:
Next, click the gear icon on the top-right, interpreter binding setting will be unfolded. Default interpreters will be enough for the most of cases, but you can add/remove at &lsquo;interpreter&rsquo; menu if you want to. Click on &lsquo;Save&rsquo; once you complete your configuration.
2. Basic usage You can click the gear icon at the right side of the paragraph. If you click &lsquo;Show title&rsquo; you can give a title as you want for each paragraph. Try to use other commands also.
Text note Like other Notebooks, e.g. Jupyter, we can put some text in a paragraph by using md command with Markdown syntax:
%md &lt;some text using markdown syntax&gt; After put text, click play button or Shift+Enter to run the paragraph. It will show formatted Markdown text. You can also choose show/hide editor for better visual or publishing.
Scala code If you bind default interpreters, you can use scala codes as well as Spark API directly in a paragraph:
Again, do not forget to click play button or Shift+Enter to actually run the paragraph.
Possible Error If you meet an error related with HDFS, please check whether you have created HDFS user folder for &lsquo;zeppelin&rsquo; as described in Zeppelin-Intro
3. Load Data Into Table We can use sql query statements for easier visualization with Zeppelin. Later, you can fully utilize Angular or D3 in Zeppelin for better or more sophisticated visualization.
Let&rsquo;s get &lsquo;Bank&rsquo; data from the official Zeppelin tutorial.
Next, define a case class for easy transformation into DataFrame and map the text data we downloaded into DataFrame without its header. Finally, register this DataFrame as Table to use sql query statements.
4. Visualization of Data via SQL query statement Once data is loaded into Table, you can use SQL query to visualize data you want to see:
%sql &lt;valid SQL statement&gt; Let&rsquo;s try to show a distribution of age of who are younger than 30.
As you can see, visualization tool will be automatically loaded once you run a paragraph with SQL statement. Default one is the result table of the query statement, but you can choose other types of visualization such as bar chart, pie chart and line chart by just clicking the icons.
Also, you can change configurations for each chart as you want
Input Form You can create input form by using \${formName} or \${formName=defaultValue} templates.
Select Form Also, you can create select form by using \${formName=defaultValue,option1|option2...}
For more dynamic forms, please refer to zeppelin-dynamicform
5. Export/Import Notebook Once you finish your works, you can export Notebook as JSON file for later use.
Also, you can import Notebook exported as JSON or from URL.
Tutorial File You can download the JSON file for this tutorial here or see the official &lsquo;Zeppelin Tutorial&rsquo; on the frontpage of Zeppelin.
`}),e.add({id:18,href:"/bigdata-bootcamp/unused/env-aws-docker/",title:"Env Aws Docker",content:"Docker in AWS EC2 We developed a Docker image which pre-installed all modules in this bootcamp. You can directly use it in your own environment if you have docker. This page describes how to launch an EC2 instance on AWS and run docker container within it.\nLaunch an AWS EC2 instance Open the Amazon EC2 console at https://console.aws.amazon.com/ec2/ From the Amazon EC2 console dashboard, click AMIs on left sidebar. Switch Region to N. Virginia if you are in other regions. Choose Public Images in dropdown below launch and search for ami-59d4f433. Select the image and click the blue Launch button. On the Choose an Instance Type page, select the hardware configuration and size of the instance to launch. Choose the type m4.xlarge, with 4 vCPUs and 16GB memory, then click “Next: Configuration Instance Details”. On the Configure Instance Details page, just keep the default settings. On the Add Storage page, you can specify storage size for your disk. Use the default 30GB. On the Tag Instance page, specify tags for your instance by providing key value combinations if you need. On the Configure Security Group page, define firewall rules for your instance. We suggest you&rsquo;d better keep the default setting unless you are sure what you are doing. On the Review Instance Launch page, check the details of your instance and click Launch. In the Select an existing key pair or create a new key pair dialog box. If you don’t have an existing key pair, choose create a new key pair. Enter a name for the key pair (e.g. bdhKeyPair) and click “Download Key Pair”. This key pair will be used to connect to your instance. Then on the same dialog box, choose Choose an existing key pair, and select the one you just created. Finally, click “Launch Instances”. You can view your instances by clicking Instances on the left navigation bar. Connect to the instance After your instance is fully launched, you can connect to it using SSH client. Right click on the instance and click connect then AWS will show you instructions about connecting on various platform. ssh command will be used for *nix platform and Putty will be used for windows.\nStart a docker container A pre-configured container with all necessary module installed is available for you to directly use. Navigate to ~/lab/docker and vagrant up will launch a container like below\nLast login: Mon Jan 25 03:29:38 2016 from lawn-128-61-36-142.lawn.gatech.edu __| __|_ ) _| ( / Amazon Linux AMI ___|\\___|___| https://aws.amazon.com/amazon-linux-ami/2015.09-release-notes/ 20 package(s) needed for security, out of 38 available Run &#34;sudo yum update&#34; to apply all updates. [ec2-user@ip-172-31-23-158 ~]$ cd lab/docker/ [ec2-user@ip-172-31-23-158 docker]$ vagrant up Bringing machine &#39;bootcamp1&#39; up with &#39;docker&#39; provider... ==&gt; bootcamp1: Creating the container... bootcamp1: Name: docker_bootcamp1_1453695135 bootcamp1: Image: sunlab/bigdata:0.04 bootcamp1: Volume: /home/ec2-user/lab/bigdata-bootcamp:/home/ec2-user/bigdata-bootcamp bootcamp1: Volume: /home/ec2-user/lab/scripts:/home/ec2-user/bigdata-scripts bootcamp1: Volume: /home/ec2-user/lab/docker:/vagrant bootcamp1: bootcamp1: Container created: cc2f518631e86a11 ==&gt; bootcamp1: Starting container... ==&gt; bootcamp1: Provisioners will not be run since container doesn&#39;t support SSH. [ec2-user@ip-172-31-23-158 docker]$ vagrant ssh Last login: Thu Jan 21 20:59:15 2016 from ip-172-17-0-1.ec2.internal [ec2-user@bootcamp1 ~]$ Then start all hadoop related service by\n[ec2-user@bootcamp1 ~]$ bigdata-scripts/start-all.sh JMX enabled by default Using config: /etc/zookeeper/conf/zoo.cfg Starting zookeeper ... STARTED starting proxyserver, logging to /var/log/hadoop-yarn/yarn-yarn-proxyserver-bootcamp.local.out Started Hadoop proxyserver: [ OK ] starting namenode, logging to /var/log/hadoop-hdfs/hadoop-hdfs-namenode-bootcamp.local.out Started Hadoop namenode: [ OK ] starting datanode, logging to /var/log/hadoop-hdfs/hadoop-hdfs-datanode-bootcamp.local.out Started Hadoop datanode (hadoop-hdfs-datanode): [ OK ] starting resourcemanager, logging to /var/log/hadoop-yarn/yarn-yarn-resourcemanager-bootcamp.local.out Started Hadoop resourcemanager: [ OK ] starting historyserver, logging to /var/log/hadoop-mapreduce/mapred-mapred-historyserver-bootcamp.local.out Started Hadoop historyserver: [ OK ] starting nodemanager, logging to /var/log/hadoop-yarn/yarn-yarn-nodemanager-bootcamp.local.out Started Hadoop nodemanager: [ OK ] Starting Spark worker (spark-worker): [ OK ] Starting Spark master (spark-master): [ OK ] Starting Hadoop HBase regionserver daemon: starting regionserver, logging to /var/log/hbase/hbase-hbase-regionserver-bootcamp.local.out hbase-regionserver. starting master, logging to /var/log/hbase/hbase-hbase-master-bootcamp.local.out Started HBase master daemon (hbase-master): [ OK ] starting thrift, logging to /var/log/hbase/hbase-hbase-thrift-bootcamp.local.out Started HBase thrift daemon (hbase-thrift): [ OK ] [ec2-user@bootcamp1 ~]$ Termination You can terminate a docker by vagrant destroy --force in ~/lab/docker/.\nLimitations After a docker container exit, you may loose data stored within it. You can map folder from AWS EC2 instance with Docker container for persistent data saving.\n"}),e.add({id:19,href:"/bigdata-bootcamp/unused/env-docker-compose/",title:"Env Docker Compose",content:`Docker Compose ::: tip This is an optional section. Docker Compose is just a utility, which does NOT affect the functionality of the docker image and our course.
You can simply ignore it if you believe the docker commands are enough. :::
Docker Compose is a tool for defining and running multi-container Docker applications. We can write a simple docker-compose.yml file as configure file, and launch a docker container as a service easily.
This means you can fix all your parameters in your configure file, and start/stop them using more biref commands.
Official Guides Compose Overview Install Compose Getting Started Compose command-line reference Compose file version 3 reference Compose file structure and examples Environment file Install Docker Compose If you are a Windows/macOS user, the docker-compose should installed with your docker application.
If this command is missing in your machine
# docker-compose -bash: docker-compose: command not found It is also pretty easy.
Please visit the Docker Compose Release Page, following the guide and download the latest binary release.
Create A docker-compose.yml You can go to an empty folder in somewhere, and create a text file named as docker-compose.yml. The content may as follow:
version: &#39;3&#39; services: bootcamp: image: sunlab/bigbox:latest hostname: bootcamp domainname: local restart: &#34;no&#34; volumes: # Volumes section defined the mappings between host machine and # virtual machine. # &#34;:&#34; split each element into 2 parts # the left part is the folder in host machine # the right part is the folder in virtual machine # docker-compose support relative path # Please refer to # https://docs.docker.com/compose/compose-file/#volumes # for more detail if you are interested - ./data/logs:/var/log - ./data/host:/mnt/host environment: - CONTAINER=docker # /scripts/entrypoint.sh will start all the services # and then finally listen to port 22. command: [ &#34;/scripts/entrypoint.sh&#34; ] ports: # Ports section defined a few rules and forward the network # stream between host machine and vm. # As the rules in volumes section # The left part is for your host machine. # This means you can visit localhost:2333 # and then get the response from the app # listening port 22 in docker container - &#34;2333:22&#34; - &#34;7077:7077&#34; # spark - &#34;4040:4040&#34; - &#34;4041:4041&#34; - &#34;8888:8888&#34; # - &#34;8983:8983&#34; # for solr Here is a example.
Basic Operation Similar to other parts. If you are a linux user, you should always add an &lsquo;sudo&rsquo; before the command docker-compose.
Up and Down, Start and Stop. How to control the virtual environment If you wish to create and start the container(s).
docker-compose up This command will search the configure file docker-compose.yml or docker-compose.yaml in current folder, and start the corresponding services.
It will pull the image described in the yml file if there is no local cache. Otherwise it will create a container through local image.
In this case, if you close the terminal, the service may be stopped but not destroyed.
You an pass a parameter -d. This parameter indices the docker-compose will run the containers in the background after started.
Please type the following command for more introduction.
docker-compose help up If you wish to clearly terminate everything. You may type
docker-compose down This command will stops containers and removes containers, networks, volumes, and images created by up.
Everything inside the container are vanished.
If you have a container created. You may use docker-compose start/stop/restart to start/stop/restart the service. The container will stay over there
Run, Exec and SSH. How to access the environment docker-compose run bootcamp bash run will launch a brand new container based on the configuration. bootcamp is name of the service in the config. This may useful in debugging or somewhere else, but may not fit for this course.
docker-compose exec bootcamp bash exec will reuse the container and execute a specific command. The command is bash in this case. You should make sure the service is already started before typing this command.
You can also access the environment through SSH.
We have also initialized the SSH RSA Keys here.
curl https://raw.githubusercontent.com/yuikns/bigbox/master/config/ssh/id_rsa -o bigbox-private-key chmod 0600 bigbox-private-key # prevent error: Permissions 0644 for &#39;./bigbox-private-key&#39; are too open. ssh -p 2333 -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -i bigbox-private-key root@127.0.0.1 If you are running your docker compose in remote. This identity file may unsafe. You can generate a pair by yourself.
[root@bootcamp ~]# rm -rf ~/.ssh/ [root@bootcamp ~]# ssh-keygen Generating public/private rsa key pair. Enter file in which to save the key (/root/.ssh/id_rsa): Created directory &#39;/root/.ssh&#39;. Enter passphrase (empty for no passphrase): # type enter here Enter same passphrase again: # type enter here again Your identification has been saved in /root/.ssh/id_rsa. Your public key has been saved in /root/.ssh/id_rsa.pub. The key fingerprint is: SHA256:2NYgKW6bCmLvFPAug868hXBAIW6PhWlJeRK6LeYCHJ0 root@bootcamp.local The key&#39;s randomart image is: +---[RSA 2048]----+ |.=+ | |*o=.. . | |oXoE. o . | |++B. . + o | |=+o+o . S . | |*oo..o . | |=* +o | |B.*. | | =+o | +----[SHA256]-----+ [root@bootcamp ~]# cd .ssh/ [root@bootcamp .ssh]# cp id_rsa.pub authorized_keys [root@bootcamp .ssh]# chmod 0600 authorized_keys [root@bootcamp .ssh]# ls -alh total 20K drwx------ 2 root root 4.0K Jun 20 08:42 . dr-xr-x--- 1 root root 4.0K Jun 20 08:41 .. -rw------- 1 root root 401 Jun 20 08:42 authorized_keys -rw------- 1 root root 1.7K Jun 20 08:41 id_rsa -rw-r--r-- 1 root root 401 Jun 20 08:41 id_rsa.pub You may copy the id_rsa file to local as your key file.
Logs Logs for Hadoop Ecosystem You may noticed, the sample docker-compose.yml mapped the folder from /var/log in vm to ./data/logs in physical machine. You may simply check the files in your folder ./data/logs.
Logs from docker container You may trace it using command:
docker-compose logs -f --tail=100 `}),e.add({id:20,href:"/bigdata-bootcamp/unused/",title:"Unuseds",content:""})})()